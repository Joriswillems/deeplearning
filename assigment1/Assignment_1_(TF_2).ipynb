{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ORVPadj7rKq"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "<b>Group 82</b>\n",
    "* <b> Student 1 </b> : Joris Willems, 0908753\n",
    "* <b> Student 2 </b> : Lars Schilders, 0908729\n",
    "\n",
    "**Reading material**\n",
    "* [1] Mikolov, Tomas, et al. \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\" Advances in neural information processing systems. 2013. \n",
    "\n",
    "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rQwLyiMFu_1"
   },
   "source": [
    "# Question 1 - Keras implementation (10 pt)\n",
    "\n",
    "### Word embeddings\n",
    "Build word embeddings with a Keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training. Use a window size of 2 to train the embeddings (`window_size` in the jupyter notebook). \n",
    "\n",
    "1. Build word embeddings of length 50, 150 and 300 using the Skipgram model\n",
    "2. Build word embeddings of length 50, 150 and 300 using CBOW model\n",
    "3. Analyze the different word embeddings:\n",
    "    - Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in the paper. Do not use existing libraries for this task such as Gensim. \n",
    "Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. $e_{x}$ denotes the embedding of word $x$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.\n",
    "    - Give at least 5 different  examples of analogies.\n",
    "    - Compare the performance on the analogy tasks between the word embeddings and briefly discuss your results.\n",
    "\n",
    "4. Discuss:\n",
    "  - Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n",
    "\n",
    "\n",
    "<b>HINT</b> See practical 3.1 for some helpful code to start this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctoyAoX1AI6T"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VOelR7BYQ1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6983,
     "status": "ok",
     "timestamp": 1589279725100,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "vCnATRPgBZEd",
    "outputId": "66766056-0360-4bcb-8240-27b98039d1a1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# other helpful libraries\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6981,
     "status": "ok",
     "timestamp": 1589279725101,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "qBNCPtOoBbB1",
    "outputId": "8cf8d48f-9b08-478c-e7a3-288bb932eaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__) #  check what version of TF is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCd0zUO1AKjY"
   },
   "source": [
    "### Import file\n",
    "\n",
    "If you use Google Colab, you need to mount your Google Drive to the notebook when you want to use files that are located in your Google Drive. Paste the authorization code, from the new tab page that opens automatically when running the cell, in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24562,
     "status": "ok",
     "timestamp": 1589279742721,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "DdjNeehKBd-a",
    "outputId": "bccb459c-8be3-467d-9598-760ae04e7084"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjIVBt8YGUaO"
   },
   "source": [
    "Navigate to the folder in which `alice.txt` is located. Make sure to start path with '/content/drive/My Drive/' if you want to load the file from your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1589279763064,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "iS0-uINUBfic",
    "outputId": "a7068012-ab31-4d87-a9b8-2ac9c37fb194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/2IMM10 - Deep Learning\n"
     ]
    }
   ],
   "source": [
    "cd '/content/drive/My Drive/Colab Notebooks/2IMM10 - Deep Learning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz8Z5gCDBhSl"
   },
   "outputs": [],
   "source": [
    "file_name = 'alice.txt'\n",
    "corpus = open(file_name).readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkbk32wHANnD"
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "See Practical 3.1 for an explanation of the preprocessing steps done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37PyOHq2BkY4"
   },
   "outputs": [],
   "source": [
    "# Removes sentences with fewer than 3 words\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# remove punctuation in text and fit tokenizer on entire corpus\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# convert text to sequence of integer values\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "n_samples = sum(len(s) for s in corpus) # total number of words in the corpus\n",
    "V = len(tokenizer.word_index) + 1 # total number of unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1589279777863,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "ILdA_IimBlte",
    "outputId": "30756520-d777-4968-ff46-d5364fc5bf4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus: 27165 \n",
      "Number of words in vocabulary: 2557\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in corpus: {} \\nNumber of words in vocabulary: {}\".format(n_samples, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1589279780456,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "fRbpue0WBms6",
    "outputId": "ebbf9719-6d1f-4a02-ec75-cdbe1ecc9589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
     ]
    }
   ],
   "source": [
    "# example of how word to integer mapping looks like in the tokenizer\n",
    "print(list((tokenizer.word_index.items()))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Er86VxH9BqI9"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "window_size = 2\n",
    "window_size_corpus = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0sU1JORATvX"
   },
   "source": [
    "## Task 1.1 - Skipgram\n",
    "Build word embeddings of length 50, 150 and 300 using the Skipgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Udp1xKcDBu0v"
   },
   "outputs": [],
   "source": [
    "#prepare data for skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # Add the input word\n",
    "                    #in_words.append(word)\n",
    "                    all_in.append(word)\n",
    "                    # Add one-hot of the context words\n",
    "                    all_out.append(to_categorical(words[i], V))\n",
    "                                      \n",
    "    return (np.array(all_in),np.array(all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXBGNrKTB0tO"
   },
   "outputs": [],
   "source": [
    "# create training data\n",
    "x , y = generate_data_skipgram(corpus,window_size,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1FTkStoDLFQ"
   },
   "outputs": [],
   "source": [
    "# create skipgram architecture\n",
    "def build_skipgram(dim):\n",
    "    skipgram = Sequential()\n",
    "    skipgram.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "    skipgram.add(Reshape((dim, )))\n",
    "    skipgram.add(Dense(V, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
    "    return skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxb3ulOlD1Z2"
   },
   "source": [
    "<b>HINT</b>: To increase training speed of your model, you can use the free available GPU power in Google Colab. Go to `Edit` --> `Notebook Settings` --> select `GPU` under `hardware accelerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144525,
     "status": "ok",
     "timestamp": 1589281183271,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "a9eg0xPoDP9B",
    "outputId": "83972df0-8bf3-459c-c7f5-33bc440368bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94556 samples\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 7.2228\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 4s 39us/sample - loss: 6.5966\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 6.3870\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 5s 52us/sample - loss: 6.2722\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 6.1929\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 6.1324\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 4s 44us/sample - loss: 6.0832\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 6.0420\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 6.0064\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.9750\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.9469\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 4s 38us/sample - loss: 5.9218\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 4s 39us/sample - loss: 5.8992\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 4s 40us/sample - loss: 5.8789\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 4s 45us/sample - loss: 5.8603\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.8432\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.8275\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.8127\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 4s 46us/sample - loss: 5.7989\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 4s 45us/sample - loss: 5.7858\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.7734\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.7615\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 4s 44us/sample - loss: 5.7501\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.7391\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.7285\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 4s 44us/sample - loss: 5.71820s - loss: 5.7\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 4s 44us/sample - loss: 5.7081\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 4s 45us/sample - loss: 5.6984\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.6888\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.6795\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.6703\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.6615\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.6530\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.6447\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.6366\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.6286\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 4s 47us/sample - loss: 5.6209\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 5s 48us/sample - loss: 5.6134\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 4s 45us/sample - loss: 5.6058\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.5983\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.5912\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.58390s - loss: 5.\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 4s 42us/sample - loss: 5.5768\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.5699\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.5628\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 4s 43us/sample - loss: 5.5559\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 4s 41us/sample - loss: 5.5490\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 4s 47us/sample - loss: 5.54230s - loss: 5.54\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 4s 47us/sample - loss: 5.5355\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 4s 46us/sample - loss: 5.5287\n",
      "Train on 94556 samples\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 6s 65us/sample - loss: 7.2213\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 6s 65us/sample - loss: 6.5931\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 6s 65us/sample - loss: 6.3805\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 6s 64us/sample - loss: 6.2631\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 7s 71us/sample - loss: 6.1803\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 6s 63us/sample - loss: 6.1156\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 5s 57us/sample - loss: 6.0629\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 6s 59us/sample - loss: 6.0185\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 5s 52us/sample - loss: 5.9804\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 5s 56us/sample - loss: 5.94720s - l\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 5s 52us/sample - loss: 5.9179\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.8921\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.8689\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.8479\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 5s 48us/sample - loss: 5.8287\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.8108\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 5s 56us/sample - loss: 5.7941\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 5s 52us/sample - loss: 5.7786\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 5s 57us/sample - loss: 5.7637\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 5s 56us/sample - loss: 5.7497\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 5s 57us/sample - loss: 5.7362\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 5s 56us/sample - loss: 5.7234\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 5s 56us/sample - loss: 5.7109\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 5s 55us/sample - loss: 5.6989\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 5s 53us/sample - loss: 5.6874\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 5s 53us/sample - loss: 5.6765\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 6s 59us/sample - loss: 5.6658\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 6s 64us/sample - loss: 5.6555\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 6s 63us/sample - loss: 5.6456\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 6s 64us/sample - loss: 5.6358\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 6s 63us/sample - loss: 5.6263\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 6s 64us/sample - loss: 5.6170\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 6s 64us/sample - loss: 5.6078\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 5s 57us/sample - loss: 5.5987\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 5s 54us/sample - loss: 5.5898\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 5s 51us/sample - loss: 5.5810\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.5725\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.5638\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.5552\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.5468\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.5386\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 5s 51us/sample - loss: 5.5303\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.5221\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 5s 49us/sample - loss: 5.5139\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 5s 48us/sample - loss: 5.5057\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.49770s - loss: \n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 5s 50us/sample - loss: 5.4898\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 6s 63us/sample - loss: 5.4817\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 6s 59us/sample - loss: 5.4740\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 6s 59us/sample - loss: 5.4660\n",
      "Train on 94556 samples\n",
      "Epoch 1/50\n",
      "94556/94556 [==============================] - 9s 92us/sample - loss: 7.2196\n",
      "Epoch 2/50\n",
      "94556/94556 [==============================] - 8s 81us/sample - loss: 6.5872\n",
      "Epoch 3/50\n",
      "94556/94556 [==============================] - 8s 81us/sample - loss: 6.3706\n",
      "Epoch 4/50\n",
      "94556/94556 [==============================] - 8s 83us/sample - loss: 6.2492\n",
      "Epoch 5/50\n",
      "94556/94556 [==============================] - 8s 82us/sample - loss: 6.1621\n",
      "Epoch 6/50\n",
      "94556/94556 [==============================] - 7s 77us/sample - loss: 6.0941\n",
      "Epoch 7/50\n",
      "94556/94556 [==============================] - 7s 76us/sample - loss: 6.0391\n",
      "Epoch 8/50\n",
      "94556/94556 [==============================] - 7s 77us/sample - loss: 5.9936\n",
      "Epoch 9/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.9550\n",
      "Epoch 10/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.9218\n",
      "Epoch 11/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.8928\n",
      "Epoch 12/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.8671\n",
      "Epoch 13/50\n",
      "94556/94556 [==============================] - 7s 77us/sample - loss: 5.8441\n",
      "Epoch 14/50\n",
      "94556/94556 [==============================] - 7s 75us/sample - loss: 5.8230\n",
      "Epoch 15/50\n",
      "94556/94556 [==============================] - 7s 77us/sample - loss: 5.8036\n",
      "Epoch 16/50\n",
      "94556/94556 [==============================] - 7s 75us/sample - loss: 5.7855\n",
      "Epoch 17/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.7686\n",
      "Epoch 18/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.7525\n",
      "Epoch 19/50\n",
      "94556/94556 [==============================] - 8s 79us/sample - loss: 5.7372\n",
      "Epoch 20/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.7227\n",
      "Epoch 21/50\n",
      "94556/94556 [==============================] - 7s 75us/sample - loss: 5.7089\n",
      "Epoch 22/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.6957\n",
      "Epoch 23/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.6831\n",
      "Epoch 24/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.6709\n",
      "Epoch 25/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.6592\n",
      "Epoch 26/50\n",
      "94556/94556 [==============================] - 7s 72us/sample - loss: 5.6480\n",
      "Epoch 27/50\n",
      "94556/94556 [==============================] - 7s 72us/sample - loss: 5.6371\n",
      "Epoch 28/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.6263\n",
      "Epoch 29/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.6159\n",
      "Epoch 30/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.6057\n",
      "Epoch 31/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5957\n",
      "Epoch 32/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.5857\n",
      "Epoch 33/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.5759\n",
      "Epoch 34/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.5664\n",
      "Epoch 35/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5569\n",
      "Epoch 36/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5475\n",
      "Epoch 37/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5381\n",
      "Epoch 38/50\n",
      "94556/94556 [==============================] - 7s 72us/sample - loss: 5.5289\n",
      "Epoch 39/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5196\n",
      "Epoch 40/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.5106\n",
      "Epoch 41/50\n",
      "94556/94556 [==============================] - 7s 74us/sample - loss: 5.5016\n",
      "Epoch 42/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.4926\n",
      "Epoch 43/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.4839\n",
      "Epoch 44/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.4752\n",
      "Epoch 45/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.4663\n",
      "Epoch 46/50\n",
      "94556/94556 [==============================] - 7s 73us/sample - loss: 5.4578\n",
      "Epoch 47/50\n",
      "94556/94556 [==============================] - 8s 87us/sample - loss: 5.4493\n",
      "Epoch 48/50\n",
      "94556/94556 [==============================] - 8s 81us/sample - loss: 5.4407\n",
      "Epoch 49/50\n",
      "94556/94556 [==============================] - 8s 81us/sample - loss: 5.4322\n",
      "Epoch 50/50\n",
      "94556/94556 [==============================] - 7s 78us/sample - loss: 5.4238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbc95b5b0d0>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train skipgram model\n",
    "skipgram_50 = build_skipgram(50)\n",
    "skipgram_50.fit(x, y, batch_size=64, epochs=50)\n",
    "skipgram_150 = build_skipgram(150)\n",
    "skipgram_150.fit(x, y, batch_size=64, epochs=50)\n",
    "skipgram_300 = build_skipgram(300)\n",
    "skipgram_300.fit(x, y, batch_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGEHlkQ4DShO"
   },
   "outputs": [],
   "source": [
    "# save embeddings for vectors of length 50, 150 and 300 using skipgram model\n",
    "embedding_50 = skipgram_50.get_weights()[0]\n",
    "embedding_150 = skipgram_150.get_weights()[0]\n",
    "embedding_300 = skipgram_300.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"embedding50.npy\",  embedding_50)\n",
    "np.save(\"embedding150.npy\", embedding_150)\n",
    "np.save(\"embedding300.npy\", embedding_300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4z9Lt6pAZEw"
   },
   "source": [
    "## Task 1.2 - CBOW\n",
    "\n",
    "Build word embeddings of length 50, 150 and 300 using CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# prepare data for CBOW\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    \n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            \n",
    "            context = [words[i] for i in range(p, n) if ((i != index) and (0 <= i < L))]\n",
    "            context = [(maxlen - len(context)) * [0] + context]\n",
    "            \n",
    "            all_in.append(context)\n",
    "\n",
    "            all_out.append(to_categorical(word, V))\n",
    "                                      \n",
    "    return (np.array(all_in),np.array(all_out))\n",
    "\n",
    "# create training data\n",
    "x_cbow, y_cbow = generate_data_cbow(corpus,window_size, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# create CBOW architecture\n",
    "def build_cbow(dim):\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim=, output_dim=, embeddings_initializer='glorot_uniform', input_length=))\n",
    "    cbow.add(Reshape(( , )))\n",
    "    cbow.add(Dense( , kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.1))\n",
    "    return skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# train CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# save embeddings for vectors of length 50, 150 and 300 using CBOW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rr2BAI4nAfnG"
   },
   "source": [
    "## Task 1.3 - Analogy function\n",
    "\n",
    "Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in [1]. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. \n",
    "\n",
    "In a perfect scenario, we would like that this analogy ( $e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. The result of the formula is called the expected or the predicted word embedding. In this context, \"man\" is called the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.  \n",
    "\n",
    "You have to answer an analogy function using each embedding for both CBOW and Skipgram model. This means that for each analogy we have 6 outputs. Show the true word (with distance similarity value between predicted embedding and true word embedding, i.e. `sim1`) , the predicted word (with distance similarity value between predicted embedding and the embedding of the word in the vocabulary that is closest to this predicted embedding, i.e. `sim2`) and a boolean answer whether the predicted word **exactly** equals the true word. \n",
    "\n",
    "<b>HINT</b>: to visualize the results of the analogy tasks , you can print them in a table. An example is given below.\n",
    "\n",
    "\n",
    "| Analogy task | True word (sim1)  | Predicted word (sim2) | Embedding | Correct?|\n",
    "|------|------|------|------|------|\n",
    "|  queen is to king as woman is to ?\t | man (sim1) | predictd_word(sim2) | SG_50 | True / False|\n",
    "\n",
    "* Give at least 5 different  examples of analogies.\n",
    "* Compare the performance on the analogy s between the word embeddings and briefly discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_embedding_50 = np.load('embedding/embedding50.npy')\n",
    "skipgram_embedding_150 = np.load('embedding/embedding150.npy')\n",
    "skipgram_embedding_300 = np.load('embedding/embedding300.npy')\n",
    "\n",
    "# cbow_embedding_50 = \n",
    "# cbow_embedding_150 = \n",
    "# cbow_embedding_300 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7vMhZ0LitFC"
   },
   "outputs": [],
   "source": [
    "def embed(word, embedding, vocab_size = V, tokenizer=tokenizer):\n",
    "    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
    "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
    "    # get the one-hot encoding of the word\n",
    "    bin_word = to_categorical(int_word, V)\n",
    "    return np.dot(bin_word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def findAnalogy(prediction, embedding):\n",
    "    \n",
    "    all_word_embeddings = embed(list(tokenizer.word_index.keys()), embedding)\n",
    "    \n",
    "    similarity = cosine_similarity(all_word_embeddings, prediction)\n",
    "\n",
    "    word_idx = similarity.argsort(axis=0)[-2,0]\n",
    "\n",
    "    return list(tokenizer.word_index.keys())[word_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_items([('chapter', 12), ('i', 545), ('down', 102), ('the', 1640), ('rabbit', 51), ('hole', 5), ('alice', 395), ('was', 356), ('beginning', 14), ('to', 726), ('get', 46), ('very', 144), ('tired', 7), ('of', 512), ('sitting', 10), ('by', 58), ('her', 248), ('sister', 9), ('on', 190), ('bank', 3), ('and', 869), ('having', 10), ('nothing', 33), ('do', 81), ('once', 34), ('or', 77), ('twice', 5), ('she', 551), ('had', 176), ('peeped', 3), ('into', 67), ('book', 11), ('reading', 3), ('but', 170), ('it', 592), ('no', 90), ('pictures', 4), ('conversations', 1), ('in', 367), ('what', 140), ('is', 108), ('use', 18), ('a', 631), ('thought', 74), ('without', 26), ('so', 151), ('considering', 3), ('own', 10), ('mind', 11), ('as', 263), ('well', 62), ('could', 75), ('for', 152), ('hot', 7), ('day', 28), ('made', 29), ('feel', 8), ('sleepy', 5), ('stupid', 6), ('whether', 11), ('pleasure', 2), ('making', 8), ('daisy', 1), ('chain', 1), ('would', 83), ('be', 148), ('worth', 4), ('trouble', 6), ('getting', 22), ('up', 99), ('picking', 2), ('daisies', 1), ('when', 79), ('suddenly', 13), ('white', 30), ('with', 181), ('pink', 1), ('eyes', 29), ('ran', 16), ('close', 13), ('there', 99), ('remarkable', 2), ('that', 314), ('nor', 3), ('did', 63), ('think', 52), ('much', 51), ('out', 117), ('way', 56), ('hear', 14), ('say', 50), ('itself', 14), ('oh', 45), ('dear', 29), ('shall', 25), ('late', 6), ('over', 39), ('afterwards', 2), ('occurred', 2), ('ought', 14), ('have', 80), ('wondered', 1), ('at', 212), ('this', 134), ('time', 71), ('all', 182), ('seemed', 27), ('quite', 55), ('natural', 4), ('actually', 1), ('took', 24), ('watch', 8), ('its', 57), ('waistcoat', 2), ('pocket', 7), ('looked', 45), ('then', 94), ('hurried', 10), ('started', 2), ('feet', 19), ('flashed', 1), ('across', 5), ('never', 48), ('before', 38), ('seen', 15), ('either', 10), ('take', 21), ('burning', 1), ('curiosity', 5), ('field', 1), ('after', 43), ('fortunately', 1), ('just', 52), ('see', 67), ('pop', 1), ('large', 33), ('under', 16), ('hedge', 2), ('another', 22), ('moment', 31), ('went', 82), ('how', 68), ('world', 7), ('again', 82), ('straight', 2), ('like', 85), ('tunnel', 1), ('some', 51), ('dipped', 2), ('not', 145), ('about', 93), ('stopping', 1), ('herself', 83), ('found', 32), ('falling', 2), ('deep', 7), ('fell', 6), ('slowly', 8), ('plenty', 2), ('look', 29), ('wonder', 18), ('going', 27), ('happen', 8), ('next', 30), ('first', 50), ('tried', 19), ('make', 27), ('coming', 9), ('too', 25), ('dark', 3), ('anything', 20), ('sides', 4), ('noticed', 8), ('they', 152), ('were', 85), ('filled', 3), ('cupboards', 2), ('shelves', 2), ('here', 51), ('saw', 14), ('maps', 1), ('hung', 1), ('upon', 26), ('pegs', 1), ('jar', 2), ('from', 35), ('one', 104), ('passed', 5), ('labelled', 1), ('orange', 1), ('marmalade', 1), ('great', 38), ('disappointment', 1), ('empty', 1), ('drop', 1), ('fear', 4), ('killing', 1), ('somebody', 7), ('managed', 4), ('put', 31), ('past', 3), ('such', 41), ('fall', 7), ('tumbling', 2), ('stairs', 3), ('brave', 1), ('ll', 57), ('me', 68), ('home', 5), ('why', 40), ('wouldn', 13), ('t', 218), ('even', 19), ('if', 96), ('off', 72), ('top', 8), ('house', 18), ('which', 49), ('likely', 5), ('true', 4), ('come', 46), ('an', 57), ('end', 18), ('many', 12), ('miles', 3), ('ve', 44), ('fallen', 4), ('said', 461), ('aloud', 4), ('must', 44), ('somewhere', 3), ('near', 15), ('centre', 1), ('earth', 4), ('let', 22), ('four', 8), ('thousand', 2), ('you', 406), ('learnt', 2), ('several', 4), ('things', 30), ('sort', 20), ('lessons', 10), ('schoolroom', 1), ('though', 11), ('good', 27), ('opportunity', 8), ('showing', 2), ('knowledge', 3), ('listen', 7), ('still', 13), ('practice', 1), ('yes', 13), ('s', 200), ('right', 32), ('distance', 7), ('latitude', 2), ('longitude', 2), ('got', 45), ('idea', 15), ('nice', 6), ('grand', 3), ('words', 21), ('presently', 2), ('began', 58), ('through', 14), ('funny', 3), ('seem', 8), ('among', 12), ('people', 13), ('walk', 5), ('their', 52), ('heads', 10), ('downward', 1), ('antipathies', 1), ('rather', 25), ('glad', 11), ('listening', 3), ('didn', 14), ('sound', 4), ('word', 10), ('ask', 10), ('them', 88), ('name', 9), ('country', 1), ('know', 83), ('please', 19), ('ma', 3), ('am', 16), ('new', 5), ('zealand', 1), ('australia', 1), ('curtsey', 1), ('spoke', 16), ('fancy', 7), ('curtseying', 1), ('re', 37), ('air', 15), ('manage', 7), ('ignorant', 1), ('little', 127), ('girl', 4), ('asking', 5), ('perhaps', 17), ('written', 6), ('else', 12), ('soon', 25), ('talking', 16), ('dinah', 14), ('miss', 4), ('night', 5), ('should', 27), ('cat', 37), ('hope', 3), ('remember', 14), ('saucer', 1), ('milk', 2), ('tea', 19), ('my', 57), ('wish', 21), ('are', 54), ('mice', 4), ('m', 62), ('afraid', 12), ('might', 28), ('catch', 4), ('bat', 3), ('mouse', 44), ('cats', 13), ('eat', 18), ('bats', 4), ('saying', 15), ('dreamy', 1), ('sometimes', 5), ('couldn', 9), ('answer', 9), ('question', 17), ('matter', 9), ('felt', 23), ('dozing', 1), ('begun', 7), ('dream', 7), ('walking', 5), ('hand', 21), ('earnestly', 2), ('now', 60), ('tell', 32), ('truth', 1), ('ever', 21), ('thump', 2), ('came', 40), ('heap', 1), ('sticks', 1), ('dry', 8), ('leaves', 6), ('bit', 14), ('hurt', 3), ('jumped', 6), ('overhead', 1), ('long', 32), ('passage', 4), ('sight', 10), ('hurrying', 1), ('lost', 3), ('away', 25), ('wind', 2), ('turned', 16), ('corner', 4), ('ears', 5), ('whiskers', 2), ('behind', 13), ('longer', 3), ('low', 15), ('hall', 9), ('lit', 1), ('row', 2), ('lamps', 1), ('hanging', 3), ('roof', 6), ('doors', 2), ('round', 41), ('locked', 1), ('been', 38), ('side', 17), ('other', 40), ('trying', 14), ('every', 11), ('door', 30), ('walked', 10), ('sadly', 4), ('middle', 7), ('wondering', 7), ('three', 28), ('legged', 2), ('table', 18), ('solid', 1), ('glass', 10), ('except', 4), ('tiny', 4), ('golden', 7), ('key', 9), ('belong', 1), ('alas', 4), ('locks', 2), ('small', 10), ('any', 39), ('rate', 9), ('open', 7), ('however', 20), ('second', 4), ('curtain', 1), ('fifteen', 1), ('inches', 6), ('high', 16), ('lock', 1), ('delight', 3), ('fitted', 1), ('opened', 10), ('led', 4), ('larger', 7), ('than', 24), ('rat', 1), ('knelt', 1), ('along', 6), ('loveliest', 1), ('garden', 16), ('longed', 2), ('wander', 1), ('those', 9), ('beds', 2), ('bright', 8), ('flowers', 2), ('cool', 2), ('fountains', 1), ('head', 49), ('doorway', 1), ('go', 50), ('poor', 27), ('shoulders', 4), ('shut', 5), ('telescope', 2), ('only', 50), ('knew', 15), ('begin', 13), ('happened', 7), ('lately', 1), ('few', 9), ('indeed', 16), ('really', 13), ('waiting', 9), ('back', 39), ('half', 23), ('hoping', 3), ('find', 20), ('rules', 3), ('shutting', 2), ('telescopes', 1), ('bottle', 10), ('certainly', 14), ('neck', 7), ('paper', 4), ('label', 2), ('drink', 7), ('beautifully', 2), ('printed', 1), ('wise', 2), ('hurry', 10), ('marked', 6), ('poison', 3), ('read', 11), ('histories', 1), ('children', 10), ('who', 63), ('burnt', 1), ('eaten', 1), ('wild', 2), ('beasts', 2), ('unpleasant', 2), ('because', 15), ('simple', 5), ('friends', 2), ('taught', 4), ('red', 3), ('poker', 1), ('will', 33), ('burn', 2), ('hold', 10), ('cut', 5), ('your', 62), ('finger', 5), ('deeply', 4), ('knife', 3), ('usually', 2), ('bleeds', 1), ('forgotten', 6), ('almost', 6), ('certain', 3), ('disagree', 1), ('sooner', 2), ('later', 3), ('ventured', 4), ('taste', 2), ('finding', 3), ('fact', 8), ('mixed', 2), ('flavour', 1), ('cherry', 1), ('tart', 1), ('custard', 1), ('pine', 1), ('apple', 1), ('roast', 1), ('turkey', 1), ('toffee', 1), ('buttered', 1), ('toast', 1), ('finished', 12), ('curious', 19), ('feeling', 7), ('ten', 6), ('face', 15), ('brightened', 2), ('size', 13), ('lovely', 2), ('waited', 10), ('minutes', 11), ('shrink', 1), ('further', 3), ('nervous', 5), ('altogether', 5), ('candle', 3), ('flame', 1), ('blown', 1), ('thing', 49), ('while', 25), ('more', 48), ('decided', 3), ('possibly', 3), ('reach', 4), ('plainly', 1), ('best', 12), ('climb', 1), ('legs', 3), ('slippery', 1), ('sat', 17), ('cried', 20), ('crying', 2), ('sharply', 4), ('advise', 1), ('leave', 9), ('minute', 20), ('generally', 7), ('gave', 15), ('advice', 2), ('seldom', 1), ('followed', 8), ('scolded', 1), ('severely', 4), ('bring', 3), ('tears', 10), ('remembered', 5), ('box', 10), ('cheated', 1), ('game', 13), ('croquet', 8), ('playing', 2), ('against', 9), ('child', 11), ('fond', 4), ('pretending', 1), ('two', 39), ('pretend', 1), ('hardly', 12), ('enough', 17), ('left', 13), ('respectable', 1), ('person', 4), ('eye', 7), ('lying', 8), ('cake', 3), ('currants', 1), ('makes', 11), ('grow', 12), ('can', 63), ('smaller', 3), ('creep', 1), ('don', 61), ('care', 4), ('happens', 5), ('ate', 1), ('anxiously', 14), ('holding', 3), ('growing', 11), ('surprised', 7), ('remained', 3), ('same', 24), ('sure', 24), ('eats', 1), ('expecting', 3), ('dull', 3), ('life', 11), ('common', 1), ('set', 14), ('work', 8), ('ii', 1), ('pool', 11), ('curiouser', 2), ('forgot', 2), ('speak', 14), ('english', 6), ('opening', 3), ('largest', 1), ('bye', 2), ('far', 13), ('shoes', 7), ('stockings', 1), ('dears', 3), ('shan', 6), ('able', 1), ('deal', 12), ('myself', 7), ('kind', 7), ('won', 26), ('want', 9), ('give', 12), ('pair', 5), ('boots', 4), ('christmas', 1), ('planning', 1), ('carrier', 1), ('sending', 2), ('presents', 2), ('odd', 1), ('directions', 3), ('foot', 10), ('esq', 1), ('hearthrug', 1), ('fender', 1), ('love', 3), ('nonsense', 6), ('struck', 2), ('nine', 5), ('hopeless', 1), ('cry', 3), ('ashamed', 2), ('yourself', 7), ('stop', 6), ('shedding', 1), ('gallons', 1), ('until', 5), ('reaching', 1), ('heard', 30), ('pattering', 3), ('hastily', 15), ('dried', 1), ('returning', 1), ('splendidly', 1), ('dressed', 1), ('kid', 5), ('gloves', 11), ('fan', 10), ('he', 125), ('trotting', 2), ('muttering', 3), ('himself', 6), ('duchess', 42), ('savage', 4), ('kept', 13), ('desperate', 1), ('ready', 8), ('help', 9), ('timid', 3), ('voice', 48), ('sir', 7), ('violently', 4), ('dropped', 5), ('skurried', 1), ('darkness', 1), ('hard', 8), ('fanning', 1), ('queer', 12), ('everything', 14), ('yesterday', 3), ('usual', 5), ('changed', 8), ('morning', 5), ('different', 9), ('ah', 5), ('puzzle', 1), ('thinking', 10), ('age', 4), ('ada', 1), ('hair', 7), ('goes', 7), ('ringlets', 2), ('mine', 10), ('doesn', 16), ('mabel', 4), ('sorts', 3), ('knows', 2), ('besides', 4), ('puzzling', 4), ('try', 12), ('used', 13), ('times', 6), ('five', 8), ('twelve', 4), ('six', 2), ('thirteen', 1), ('seven', 6), ('twenty', 3), ('multiplication', 1), ('signify', 1), ('geography', 1), ('london', 1), ('capital', 4), ('paris', 2), ('rome', 2), ('wrong', 5), ('doth', 3), ('crossed', 3), ('hands', 12), ('lap', 2), ('repeat', 7), ('sounded', 5), ('hoarse', 3), ('strange', 5), ('crocodile', 1), ('improve', 1), ('his', 96), ('shining', 1), ('tail', 9), ('pour', 1), ('waters', 1), ('nile', 1), ('scale', 1), ('cheerfully', 1), ('seems', 5), ('grin', 6), ('neatly', 2), ('spread', 3), ('claws', 2), ('welcome', 1), ('fishes', 1), ('gently', 3), ('smiling', 2), ('jaws', 2), ('live', 8), ('poky', 1), ('toys', 1), ('play', 8), ('learn', 7), ('stay', 5), ('putting', 2), ('being', 19), ('till', 21), ('sudden', 5), ('burst', 1), ('alone', 4), ('done', 15), ('measure', 1), ('nearly', 11), ('guess', 3), ('shrinking', 4), ('rapidly', 2), ('cause', 3), ('avoid', 1), ('narrow', 2), ('escape', 4), ('frightened', 7), ('change', 14), ('existence', 1), ('speed', 1), ('worse', 3), ('declare', 2), ('bad', 2), ('these', 14), ('slipped', 3), ('splash', 1), ('chin', 6), ('salt', 2), ('water', 5), ('somehow', 1), ('sea', 12), ('case', 5), ('railway', 2), ('seaside', 1), ('general', 3), ('conclusion', 2), ('wherever', 2), ('coast', 1), ('number', 5), ('bathing', 1), ('machines', 1), ('digging', 4), ('sand', 1), ('wooden', 1), ('spades', 1), ('lodging', 1), ('houses', 1), ('station', 1), ('wept', 1), ('hadn', 8), ('swam', 5), ('punished', 1), ('suppose', 14), ('drowned', 1), ('something', 17), ('splashing', 2), ('nearer', 5), ('walrus', 1), ('hippopotamus', 1), ('talk', 14), ('harm', 1), ('o', 6), ('swimming', 2), ('speaking', 5), ('brother', 1), ('latin', 1), ('grammar', 1), ('inquisitively', 1), ('wink', 2), ('understand', 6), ('daresay', 1), ('french', 4), ('william', 8), ('conqueror', 2), ('history', 7), ('clear', 2), ('notion', 3), ('ago', 2), ('ou', 1), ('est', 1), ('chatte', 1), ('sentence', 5), ('lesson', 3), ('leap', 1), ('quiver', 1), ('fright', 2), ('beg', 8), ('pardon', 6), ('animal', 2), ('feelings', 2), ('shrill', 5), ('passionate', 1), ('soothing', 1), ('tone', 40), ('angry', 5), ('yet', 24), ('show', 3), ('our', 8), ('d', 29), ('quiet', 2), ('lazily', 1), ('sits', 1), ('purring', 2), ('nicely', 2), ('fire', 4), ('licking', 1), ('paws', 4), ('washing', 3), ('soft', 1), ('nurse', 3), ('catching', 2), ('bristling', 1), ('offended', 9), ('we', 34), ('trembling', 6), ('subject', 6), ('family', 1), ('always', 13), ('hated', 1), ('nasty', 1), ('vulgar', 1), ('conversation', 9), ('dogs', 3), ('eagerly', 8), ('dog', 3), ('eyed', 1), ('terrier', 1), ('curly', 1), ('brown', 2), ('fetch', 7), ('throw', 3), ('sit', 8), ('dinner', 2), ('belongs', 2), ('farmer', 1), ('says', 4), ('useful', 2), ('hundred', 1), ('pounds', 1), ('kills', 1), ('rats', 1), ('sorrowful', 2), ('commotion', 1), ('called', 15), ('softly', 1), ('pale', 4), ('passion', 3), ('us', 14), ('shore', 3), ('hate', 2), ('crowded', 5), ('birds', 10), ('animals', 4), ('duck', 4), ('dodo', 13), ('lory', 6), ('eaglet', 3), ('creatures', 10), ('whole', 13), ('party', 10), ('iii', 1), ('caucus', 3), ('race', 6), ('tale', 4), ('looking', 32), ('assembled', 2), ('draggled', 1), ('feathers', 1), ('fur', 3), ('clinging', 1), ('dripping', 1), ('wet', 2), ('cross', 3), ('uncomfortable', 4), ('course', 25), ('consultation', 1), ('familiarly', 1), ('known', 1), ('argument', 4), ('last', 33), ('sulky', 3), ('older', 2), ('better', 13), ('allow', 3), ('knowing', 2), ('old', 19), ('positively', 1), ('refused', 1), ('authority', 2), ('ring', 2), ('fixed', 1), ('cold', 1), ('ahem', 1), ('important', 7), ('driest', 1), ('silence', 14), ('whose', 2), ('favoured', 1), ('pope', 1), ('submitted', 1), ('wanted', 4), ('leaders', 1), ('accustomed', 1), ('usurpation', 1), ('conquest', 1), ('edwin', 2), ('morcar', 2), ('earls', 2), ('mercia', 2), ('northumbria', 2), ('ugh', 2), ('shiver', 1), ('frowning', 4), ('politely', 6), ('proceed', 2), ('declared', 1), ('him', 43), ('stigand', 1), ('patriotic', 1), ('archbishop', 1), ('canterbury', 1), ('advisable', 2), ('replied', 29), ('crossly', 1), ('means', 4), ('frog', 3), ('worm', 1), ('notice', 5), ('hurriedly', 2), ('edgar', 1), ('atheling', 1), ('meet', 2), ('offer', 2), ('crown', 3), ('conduct', 1), ('moderate', 1), ('insolence', 1), ('normans', 1), ('continued', 9), ('turning', 12), ('melancholy', 6), ('solemnly', 4), ('rising', 1), ('move', 3), ('meeting', 1), ('adjourn', 1), ('immediate', 1), ('adoption', 1), ('energetic', 1), ('meaning', 8), ('believe', 9), ('bent', 1), ('hide', 1), ('smile', 2), ('paused', 1), ('inclined', 1), ('explain', 10), ('winter', 1), ('circle', 1), ('exact', 1), ('shape', 1), ('placed', 1), ('running', 8), ('liked', 6), ('easy', 2), ('hour', 2), ('panting', 2), ('has', 7), ('pressed', 3), ('forehead', 2), ('position', 2), ('shakespeare', 1), ('rest', 10), ('everybody', 8), ('prizes', 5), ('chorus', 6), ('voices', 3), ('asked', 17), ('pointing', 4), ('calling', 1), ('confused', 4), ('despair', 1), ('pulled', 1), ('comfits', 2), ('luckily', 2), ('handed', 3), ('exactly', 8), ('piece', 6), ('prize', 1), ('gravely', 3), ('thimble', 4), ('presented', 1), ('acceptance', 1), ('elegant', 1), ('short', 4), ('speech', 3), ('cheered', 3), ('absurd', 2), ('grave', 3), ('dare', 5), ('laugh', 1), ('simply', 3), ('bowed', 4), ('solemn', 3), ('caused', 2), ('noise', 3), ('confusion', 5), ('complained', 1), ('theirs', 1), ('ones', 1), ('choked', 3), ('patted', 1), ('begged', 1), ('promised', 1), ('c', 1), ('added', 23), ('whisper', 3), ('sad', 3), ('call', 9), ('fury', 3), ('met', 3), ('both', 14), ('law', 2), ('prosecute', 1), ('denial', 1), ('trial', 10), ('cur', 1), ('jury', 22), ('judge', 4), ('wasting', 2), ('breath', 4), ('cunning', 1), ('condemn', 1), ('death', 1), ('attending', 3), ('humbly', 2), ('fifth', 1), ('bend', 2), ('angrily', 9), ('knot', 2), ('undo', 1), ('insult', 1), ('mean', 10), ('pleaded', 3), ('easily', 3), ('growled', 1), ('reply', 4), ('finish', 5), ('story', 8), ('others', 6), ('joined', 3), ('shook', 9), ('impatiently', 5), ('quicker', 1), ('pity', 3), ('sighed', 5), ('crab', 3), ('daughter', 1), ('lose', 1), ('temper', 5), ('tongue', 4), ('young', 5), ('snappishly', 1), ('patience', 1), ('oyster', 1), ('addressing', 1), ('nobody', 8), ('particular', 4), ('venture', 3), ('pet', 1), ('bird', 2), ('sensation', 2), ('magpie', 1), ('wrapping', 1), ('carefully', 3), ('remarking', 3), ('suit', 3), ('throat', 2), ('canary', 1), ('bed', 1), ('various', 1), ('pretexts', 1), ('moved', 4), ('mentioned', 3), ('lonely', 2), ('spirited', 1), ('footsteps', 2), ('iv', 1), ('sends', 1), ('bill', 17), ('executed', 6), ('ferrets', 2), ('where', 15), ('guessed', 3), ('naturedly', 1), ('hunting', 3), ('nowhere', 2), ('since', 4), ('swim', 5), ('vanished', 4), ('completely', 1), ('mary', 4), ('ann', 4), ('doing', 5), ('run', 4), ('quick', 2), ('direction', 5), ('pointed', 1), ('mistake', 3), ('housemaid', 1), ('finds', 1), ('neat', 1), ('brass', 1), ('plate', 3), ('w', 1), ('engraved', 1), ('knocking', 3), ('upstairs', 1), ('lest', 1), ('real', 3), ('messages', 2), ('fancying', 1), ('directly', 2), ('ordering', 2), ('tidy', 1), ('room', 13), ('window', 7), ('hoped', 1), ('pairs', 1), ('stood', 7), ('nevertheless', 1), ('uncorked', 1), ('lips', 1), ('interesting', 5), ('whenever', 1), ('does', 9), ('expected', 1), ('drunk', 2), ('pressing', 1), ('ceiling', 1), ('stoop', 2), ('save', 1), ('broken', 6), ('kneel', 1), ('floor', 3), ('effect', 3), ('elbow', 2), ('arm', 15), ('curled', 2), ('resource', 1), ('chimney', 6), ('whatever', 3), ('become', 5), ('magic', 1), ('full', 6), ('grew', 1), ('chance', 4), ('unhappy', 2), ('pleasanter', 1), ('wasn', 11), ('ordered', 4), ('rabbits', 1), ('gone', 13), ('fairy', 1), ('tales', 1), ('fancied', 2), ('write', 6), ('grown', 7), ('least', 9), ('comfort', 1), ('woman', 2), ('shouldn', 5), ('foolish', 1), ('answered', 4), ('books', 2), ('taking', 5), ('outside', 4), ('stopped', 3), ('trembled', 2), ('forgetting', 3), ('reason', 9), ('inwards', 1), ('attempt', 1), ('proved', 2), ('failure', 1), ('snatch', 2), ('shriek', 5), ('crash', 3), ('concluded', 2), ('possible', 1), ('cucumber', 2), ('frame', 1), ('pat', 3), ('apples', 2), ('yer', 4), ('honour', 4), ('sounds', 4), ('pronounced', 1), ('arrum', 1), ('goose', 2), ('fills', 1), ('business', 7), ('whispers', 1), ('coward', 1), ('shrieks', 1), ('frames', 1), ('pulling', 1), ('hearing', 4), ('rumbling', 1), ('cartwheels', 1), ('together', 9), ('ladder', 1), ('lad', 1), ('em', 3), ('tie', 1), ('rope', 1), ('bear', 2), ('loose', 1), ('slate', 4), ('below', 3), ('loud', 5), ('nay', 1), ('master', 4), ('shy', 1), ('place', 8), ('fireplace', 1), ('kick', 3), ('drew', 5), ('scratching', 1), ('scrambling', 1), ('above', 3), ('sharp', 6), ('brandy', 1), ('choke', 1), ('fellow', 3), ('feeble', 2), ('squeaking', 2), ('thank', 4), ('ye', 1), ('flustered', 1), ('comes', 2), ('jack', 1), ('sky', 5), ('rocket', 1), ('dead', 4), ('instantly', 5), ('sense', 3), ('moving', 3), ('barrowful', 2), ('doubt', 4), ('shower', 2), ('pebbles', 2), ('rattling', 2), ('hit', 2), ('shouted', 9), ('produced', 1), ('surprise', 5), ('cakes', 3), ('lay', 4), ('swallowed', 1), ('delighted', 2), ('crowd', 4), ('lizard', 6), ('held', 4), ('guinea', 6), ('pigs', 6), ('giving', 2), ('rush', 2), ('appeared', 8), ('safe', 2), ('thick', 1), ('wood', 8), ('wandered', 2), ('plan', 4), ('excellent', 2), ('arranged', 1), ('difficulty', 4), ('smallest', 2), ('peering', 1), ('trees', 7), ('bark', 2), ('enormous', 1), ('puppy', 7), ('feebly', 1), ('stretching', 2), ('paw', 3), ('touch', 1), ('coaxing', 2), ('whistle', 1), ('terribly', 1), ('hungry', 3), ('spite', 1), ('picked', 3), ('stick', 4), ('whereupon', 1), ('yelp', 1), ('rushed', 1), ('worry', 1), ('dodged', 1), ('thistle', 2), ('keep', 11), ('tumbled', 1), ('heels', 1), ('cart', 1), ('horse', 1), ('trampled', 1), ('series', 1), ('charges', 1), ('forwards', 1), ('each', 8), ('barking', 1), ('hoarsely', 1), ('mouth', 10), ('faint', 1), ('leant', 1), ('buttercup', 1), ('fanned', 1), ('teaching', 1), ('tricks', 1), ('blades', 1), ('grass', 4), ('circumstances', 1), ('mushroom', 8), ('height', 5), ('stretched', 2), ('tiptoe', 2), ('edge', 3), ('immediately', 3), ('caterpillar', 28), ('arms', 6), ('folded', 3), ('quietly', 5), ('smoking', 2), ('hookah', 5), ('v', 1), ('addressed', 2), ('languid', 1), ('encouraging', 2), ('shyly', 1), ('present', 3), ('sternly', 1), ('clearly', 1), ('sizes', 1), ('confusing', 3), ('isn', 7), ('haven', 8), ('turn', 11), ('chrysalis', 1), ('butterfly', 1), ('may', 13), ('contemptuously', 2), ('brought', 3), ('irritated', 1), ('remarks', 3), ('state', 1), ('promising', 1), ('swallowing', 1), ('anger', 2), ('wait', 1), ('puffed', 1), ('unfolded', 2), ('busy', 2), ('bee', 1), ('father', 6), ('man', 5), ('incessantly', 1), ('stand', 6), ('youth', 6), ('son', 1), ('feared', 1), ('injure', 1), ('brain', 1), ('perfectly', 4), ('none', 4), ('most', 8), ('uncommonly', 1), ('fat', 1), ('somersault', 2), ('pray', 3), ('sage', 1), ('grey', 1), ('limbs', 1), ('supple', 1), ('ointment', 1), ('shilling', 1), ('sell', 2), ('couple', 1), ('weak', 2), ('tougher', 1), ('suet', 1), ('bones', 1), ('beak', 1), ('argued', 1), ('wife', 1), ('muscular', 1), ('strength', 1), ('jaw', 1), ('lasted', 1), ('steady', 1), ('balanced', 1), ('eel', 2), ('nose', 8), ('awfully', 1), ('clever', 2), ('questions', 4), ('airs', 1), ('stuff', 4), ('timidly', 9), ('altered', 1), ('decidedly', 4), ('changing', 2), ('often', 5), ('contradicted', 1), ('losing', 1), ('content', 1), ('wretched', 2), ('rearing', 1), ('upright', 1), ('piteous', 1), ('patiently', 2), ('chose', 2), ('yawned', 2), ('crawled', 1), ('merely', 2), ('taller', 2), ('thoughtfully', 4), ('difficult', 2), ('broke', 2), ('nibbled', 2), ('violent', 2), ('blow', 2), ('underneath', 1), ('closely', 1), ('swallow', 1), ('morsel', 1), ('free', 3), ('alarm', 2), ('immense', 1), ('length', 1), ('rise', 1), ('stalk', 1), ('green', 4), ('result', 1), ('follow', 2), ('shaking', 3), ('distant', 2), ('serpent', 9), ('succeeded', 3), ('curving', 1), ('graceful', 1), ('zigzag', 1), ('dive', 1), ('tops', 1), ('wandering', 2), ('hiss', 1), ('draw', 7), ('pigeon', 12), ('flown', 1), ('beating', 2), ('wings', 1), ('screamed', 4), ('indignantly', 4), ('repeated', 9), ('subdued', 1), ('sob', 1), ('roots', 2), ('banks', 1), ('hedges', 1), ('serpents', 3), ('pleasing', 1), ('puzzled', 9), ('hatching', 1), ('eggs', 5), ('sleep', 6), ('weeks', 1), ('sorry', 1), ('annoyed', 1), ('taken', 4), ('highest', 1), ('tree', 8), ('raising', 1), ('needs', 1), ('wriggling', 1), ('doubtfully', 2), ('changes', 2), ('deepest', 1), ('contempt', 1), ('girls', 3), ('denying', 1), ('telling', 2), ('tasted', 3), ('truthful', 1), ('silent', 6), ('adding', 1), ('matters', 2), ('yours', 2), ('raw', 1), ('settled', 3), ('nest', 1), ('crouched', 1), ('entangled', 2), ('branches', 2), ('untwist', 1), ('pieces', 3), ('nibbling', 3), ('shorter', 1), ('bringing', 3), ('beautiful', 13), ('whoever', 1), ('lives', 4), ('frighten', 1), ('wits', 1), ('righthand', 1), ('vi', 1), ('pig', 11), ('pepper', 8), ('footman', 14), ('livery', 3), ('considered', 3), ('otherwise', 4), ('judging', 1), ('fish', 8), ('rapped', 1), ('loudly', 3), ('knuckles', 1), ('footmen', 1), ('powdered', 1), ('crept', 1), ('producing', 1), ('letter', 3), ('invitation', 2), ('queen', 72), ('order', 3), ('curls', 1), ('laughed', 2), ('ground', 7), ('staring', 3), ('stupidly', 1), ('knocked', 1), ('reasons', 1), ('secondly', 2), ('inside', 2), ('extraordinary', 2), ('within', 2), ('constant', 2), ('howling', 3), ('sneezing', 6), ('dish', 4), ('kettle', 1), ('between', 6), ('instance', 3), ('knock', 1), ('uncivil', 1), ('remarked', 10), ('tomorrow', 1), ('skimming', 1), ('grazed', 1), ('maybe', 2), ('louder', 1), ('told', 6), ('dreadful', 2), ('muttered', 2), ('argue', 1), ('drive', 2), ('crazy', 1), ('repeating', 3), ('remark', 10), ('variations', 1), ('days', 4), ('whistling', 1), ('desperately', 1), ('idiotic', 1), ('kitchen', 4), ('smoke', 1), ('stool', 1), ('nursing', 3), ('baby', 14), ('cook', 13), ('leaning', 2), ('stirring', 2), ('cauldron', 2), ('soup', 18), ('sneezed', 1), ('occasionally', 1), ('alternately', 1), ('pause', 2), ('sneeze', 2), ('hearth', 1), ('grinning', 1), ('ear', 6), ('manners', 1), ('grins', 1), ('cheshire', 7), ('violence', 1), ('courage', 3), ('grinned', 3), ('pleased', 7), ('introduce', 1), ('fix', 1), ('throwing', 2), ('irons', 1), ('saucepans', 1), ('plates', 2), ('dishes', 2), ('already', 2), ('impossible', 2), ('blows', 1), ('jumping', 4), ('agony', 1), ('terror', 1), ('precious', 1), ('unusually', 1), ('saucepan', 1), ('flew', 1), ('carried', 4), ('minded', 1), ('growl', 3), ('faster', 3), ('advantage', 3), ('takes', 2), ('hours', 4), ('axis', 1), ('axes', 1), ('chop', 1), ('glanced', 1), ('meant', 5), ('hint', 2), ('busily', 4), ('bother', 1), ('abide', 1), ('figures', 1), ('singing', 2), ('lullaby', 1), ('shake', 1), ('roughly', 1), ('boy', 3), ('beat', 4), ('sneezes', 2), ('annoy', 1), ('teases', 1), ('wow', 6), ('sang', 2), ('verse', 3), ('song', 7), ('tossing', 3), ('howled', 1), ('thoroughly', 2), ('enjoy', 1), ('pleases', 1), ('flinging', 1), ('threw', 2), ('frying', 1), ('pan', 1), ('missed', 2), ('caught', 3), ('shaped', 3), ('creature', 4), ('star', 1), ('snorting', 1), ('steam', 1), ('engine', 1), ('doubling', 1), ('straightening', 1), ('proper', 3), ('twist', 2), ('tight', 1), ('prevent', 1), ('undoing', 1), ('kill', 1), ('murder', 1), ('grunted', 4), ('grunt', 1), ('expressing', 1), ('snout', 1), ('also', 2), ('extremely', 2), ('sobbing', 3), ('seriously', 1), ('sobbed', 1), ('neither', 2), ('less', 4), ('carry', 1), ('relieved', 1), ('trot', 1), ('dreadfully', 6), ('ugly', 2), ('handsome', 1), ('startled', 2), ('seeing', 1), ('bough', 1), ('yards', 1), ('natured', 1), ('teeth', 1), ('treated', 1), ('respect', 1), ('puss', 1), ('wider', 1), ('depends', 1), ('explanation', 2), ('denied', 2), ('waving', 5), ('hatter', 56), ('march', 33), ('hare', 30), ('visit', 1), ('mad', 14), ('grant', 1), ('growls', 1), ('wags', 1), ('wag', 1), ('therefore', 1), ('growling', 1), ('invited', 2), ('happening', 1), ('became', 2), ('appear', 2), ('hatters', 1), ('raving', 2), ('branch', 1), ('fig', 1), ('appearing', 1), ('vanishing', 1), ('giddy', 2), ('ending', 2), ('farther', 1), ('chimneys', 1), ('thatched', 1), ('lefthand', 1), ('raised', 2), ('towards', 1), ('instead', 3), ('vii', 1), ('front', 2), ('dormouse', 40), ('fast', 4), ('asleep', 8), ('using', 2), ('cushion', 2), ('resting', 2), ('elbows', 1), ('chair', 1), ('wine', 2), ('civil', 3), ('laid', 2), ('wants', 2), ('cutting', 1), ('personal', 2), ('severity', 1), ('rude', 2), ('wide', 2), ('raven', 1), ('writing', 6), ('desk', 1), ('fun', 3), ('riddles', 2), ('breathe', 3), ('ravens', 1), ('desks', 1), ('break', 2), ('month', 2), ('uneasily', 2), ('fourth', 1), ('butter', 8), ('works', 1), ('meekly', 2), ('crumbs', 4), ('grumbled', 1), ('bread', 6), ('gloomily', 1), ('cup', 2), ('shoulder', 4), ('tells', 2), ('clock', 5), ('year', 2), ('readily', 1), ('stays', 1), ('poured', 1), ('riddle', 1), ('slightest', 1), ('wearily', 1), ('waste', 1), ('answers', 1), ('cautiously', 3), ('music', 3), ('accounts', 1), ('terms', 1), ('twinkling', 4), ('mournfully', 1), ('quarrelled', 1), ('spoon', 2), ('concert', 2), ('given', 1), ('hearts', 8), ('sing', 6), ('twinkle', 8), ('fly', 3), ('tray', 1), ('pinch', 2), ('bawled', 1), ('murdering', 1), ('exclaimed', 6), ('mournful', 1), ('sigh', 4), ('wash', 2), ('whiles', 1), ('interrupted', 9), ('yawning', 2), ('vote', 1), ('lady', 3), ('alarmed', 1), ('wake', 2), ('pinched', 2), ('fellows', 1), ('sisters', 2), ('names', 2), ('elsie', 1), ('lacie', 1), ('tillie', 1), ('lived', 3), ('bottom', 4), ('interest', 1), ('eating', 1), ('drinking', 1), ('treacle', 7), ('ill', 2), ('ways', 1), ('living', 2), ('opinion', 1), ('triumphantly', 2), ('helped', 1), ('sh', 2), ('sulkily', 2), ('interrupt', 1), ('consented', 1), ('learning', 2), ('promise', 1), ('clean', 1), ('unwillingly', 1), ('upset', 3), ('jug', 1), ('offend', 1), ('eh', 1), ('choosing', 1), ('interrupting', 2), ('rubbing', 2), ('manner', 2), ('begins', 4), ('closed', 2), ('doze', 1), ('woke', 1), ('traps', 1), ('moon', 1), ('memory', 1), ('muchness', 3), ('drawing', 1), ('rudeness', 1), ('disgust', 1), ('stupidest', 1), ('leading', 1), ('today', 1), ('unlocking', 1), ('flower', 2), ('viii', 1), ('rose', 4), ('entrance', 1), ('roses', 2), ('gardeners', 8), ('painting', 2), ('paint', 1), ('jogged', 1), ('blame', 1), ('deserved', 1), ('beheaded', 3), ('spoken', 1), ('tulip', 1), ('onions', 1), ('flung', 1), ('brush', 1), ('unjust', 1), ('chanced', 1), ('watching', 3), ('checked', 3), ('afore', 1), ('themselves', 2), ('flat', 2), ('faces', 5), ('eager', 3), ('soldiers', 10), ('carrying', 2), ('clubs', 1), ('oblong', 1), ('corners', 1), ('courtiers', 2), ('ornamented', 2), ('diamonds', 1), ('royal', 2), ('merrily', 1), ('couples', 1), ('guests', 3), ('mostly', 2), ('kings', 1), ('queens', 1), ('recognised', 1), ('noticing', 1), ('knave', 8), ('king', 62), ('crimson', 2), ('velvet', 1), ('procession', 5), ('doubtful', 2), ('lie', 2), ('rule', 5), ('processions', 1), ('opposite', 1), ('smiled', 2), ('idiot', 1), ('majesty', 12), ('pack', 5), ('cards', 3), ('needn', 3), ('rosetree', 1), ('pattern', 1), ('backs', 1), ('glaring', 1), ('beast', 1), ('consider', 4), ('bowing', 1), ('humble', 1), ('knee', 5), ('meanwhile', 1), ('examining', 1), ('remaining', 1), ('execute', 1), ('unfortunate', 3), ('protection', 1), ('pot', 1), ('marched', 1), ('evidently', 1), ('roared', 1), ('fine', 2), ('peeping', 1), ('hush', 3), ('whispered', 5), ('execution', 3), ('boxed', 1), ('scream', 2), ('laughter', 1), ('places', 2), ('thunder', 1), ('ridges', 1), ('furrows', 1), ('balls', 1), ('hedgehogs', 3), ('mallets', 1), ('flamingoes', 2), ('double', 1), ('arches', 4), ('chief', 1), ('managing', 1), ('flamingo', 5), ('body', 2), ('tucked', 3), ('comfortably', 1), ('straightened', 1), ('hedgehog', 7), ('expression', 1), ('bursting', 1), ('laughing', 2), ('provoking', 1), ('unrolled', 2), ('act', 1), ('crawling', 1), ('ridge', 1), ('furrow', 1), ('send', 1), ('doubled', 1), ('parts', 1), ('players', 4), ('played', 1), ('turns', 3), ('quarrelling', 2), ('fighting', 1), ('furious', 1), ('stamping', 2), ('shouting', 2), ('uneasy', 1), ('dispute', 2), ('beheading', 1), ('appearance', 1), ('nodded', 1), ('account', 1), ('someone', 1), ('fairly', 1), ('complaining', 1), ('quarrel', 1), ('oneself', 1), ('attends', 1), ('alive', 2), ('arch', 1), ('croqueted', 1), ('win', 1), ('finishing', 1), ('friend', 2), ('kiss', 1), ('likes', 1), ('impertinent', 1), ('removed', 2), ('passing', 1), ('settling', 1), ('difficulties', 1), ('executioner', 6), ('screaming', 1), ('search', 1), ('engaged', 1), ('fight', 2), ('croqueting', 1), ('helpless', 1), ('collected', 2), ('appealed', 1), ('settle', 1), ('arguments', 1), ('unless', 2), ('weren', 1), ('anxious', 3), ('prison', 1), ('arrow', 1), ('fading', 1), ('entirely', 2), ('disappeared', 1), ('wildly', 2), ('ix', 1), ('mock', 56), ('turtle', 57), ('affectionately', 1), ('pleasant', 1), ('hopeful', 1), ('tempered', 2), ('vinegar', 1), ('sour', 1), ('camomile', 1), ('bitter', 1), ('barley', 1), ('sugar', 2), ('sweet', 1), ('stingy', 1), ('forget', 2), ('moral', 8), ('hasn', 2), ('tut', 2), ('squeezed', 1), ('closer', 1), ('keeping', 2), ('uncomfortably', 1), ('bore', 1), ('tis', 5), ('minding', 1), ('morals', 1), ('waist', 1), ('experiment', 2), ('bite', 2), ('mustard', 3), ('feather', 1), ('flock', 1), ('mineral', 1), ('agree', 2), ('attended', 1), ('vegetable', 1), ('imagine', 2), ('cheap', 1), ('birthday', 1), ('dig', 1), ('worried', 1), ('died', 1), ('favourite', 1), ('linked', 1), ('hers', 4), ('tremble', 1), ('thunderstorm', 1), ('fair', 1), ('warning', 1), ('choice', 2), ('absence', 1), ('shade', 1), ('delay', 1), ('cost', 1), ('whom', 1), ('sentenced', 1), ('custody', 2), ('company', 1), ('pardoned', 1), ('executions', 2), ('gryphon', 53), ('sun', 2), ('picture', 1), ('lazy', 1), ('leaving', 1), ('rubbed', 1), ('watched', 2), ('chuckled', 1), ('executes', 1), ('ledge', 1), ('rock', 1), ('sighing', 2), ('heart', 2), ('pitied', 1), ('sorrow', 2), ('hollow', 1), ('occasional', 1), ('exclamation', 1), ('hjckrrh', 1), ('heavy', 1), ('calmly', 1), ('school', 6), ('tortoise', 3), ('sink', 1), ('mayn', 1), ('educations', 1), ('proud', 2), ('extras', 1), ('learned', 1), ('relief', 2), ('ours', 1), ('extra', 1), ('afford', 1), ('regular', 2), ('inquired', 1), ('reeling', 1), ('writhing', 1), ('arithmetic', 1), ('ambition', 1), ('distraction', 1), ('uglification', 2), ('derision', 1), ('lifted', 1), ('uglifying', 1), ('beautify', 1), ('prettier', 1), ('uglify', 1), ('simpleton', 1), ('encouraged', 1), ('mystery', 2), ('counting', 1), ('subjects', 1), ('flappers', 1), ('ancient', 1), ('modern', 1), ('seaography', 1), ('drawling', 3), ('conger', 1), ('week', 3), ('fainting', 1), ('coils', 1), ('stiff', 1), ('classics', 1), ('grief', 1), ('hid', 1), ('lessen', 1), ('eleventh', 1), ('twelfth', 1), ('games', 1), ('x', 1), ('lobster', 7), ('quadrille', 4), ('flapper', 1), ('sobs', 3), ('bone', 1), ('punching', 1), ('recovered', 2), ('cheeks', 1), ('introduced', 1), ('delightful', 2), ('dance', 13), ('form', 1), ('line', 1), ('lines', 1), ('seals', 1), ('turtles', 2), ('salmon', 1), ('cleared', 1), ('jelly', 1), ('advance', 3), ('partner', 1), ('partners', 1), ('lobsters', 7), ('retire', 1), ('bound', 1), ('capering', 1), ('yelled', 1), ('land', 1), ('figure', 3), ('dropping', 1), ('pretty', 1), ('dancing', 2), ('treading', 2), ('toes', 3), ('forepaws', 1), ('mark', 3), ('whiting', 8), ('snail', 3), ('porpoise', 4), ('shingle', 1), ('join', 9), ('askance', 1), ('thanked', 1), ('kindly', 2), ('scaly', 1), ('england', 1), ('france', 1), ('beloved', 1), ('dinn', 2), ('tails', 3), ('mouths', 4), ('thrown', 1), ('shiny', 1), ('blacking', 1), ('soles', 1), ('eels', 1), ('shrimp', 1), ('thoughts', 2), ('obliged', 3), ('anywhere', 1), ('journey', 1), ('purpose', 1), ('adventures', 6), ('impatient', 1), ('explanations', 1), ('gained', 1), ('listeners', 1), ('part', 2), ('sluggard', 1), ('baked', 1), ('eyelids', 1), ('trims', 1), ('belt', 1), ('buttons', 1), ('editions', 2), ('follows', 3), ('sands', 1), ('gay', 1), ('lark', 1), ('contemptuous', 1), ('tones', 2), ('shark', 1), ('tide', 1), ('rises', 1), ('sharks', 1), ('around', 3), ('tremulous', 1), ('explained', 1), ('persisted', 2), ('disobey', 1), ('owl', 3), ('panther', 3), ('sharing', 1), ('pie', 3), ('crust', 1), ('gravy', 1), ('meat', 1), ('share', 1), ('treat', 1), ('boon', 1), ('permitted', 1), ('received', 1), ('fork', 1), ('banquet', 1), ('hm', 1), ('accounting', 1), ('tastes', 1), ('rich', 1), ('tureen', 1), ('dainties', 1), ('evening', 5), ('beau', 4), ('ootiful', 4), ('soo', 7), ('oop', 7), ('e', 6), ('cares', 2), ('pennyworth', 2), ('beauti', 1), ('ful', 1), ('panted', 1), ('faintly', 1), ('breeze', 1), ('xi', 1), ('stole', 2), ('tarts', 7), ('seated', 1), ('throne', 1), ('arrived', 1), ('standing', 1), ('chains', 1), ('soldier', 1), ('guard', 1), ('trumpet', 3), ('scroll', 2), ('parchment', 2), ('court', 17), ('refreshments', 1), ('pass', 1), ('justice', 1), ('wig', 2), ('wore', 1), ('frontispiece', 1), ('comfortable', 1), ('becoming', 1), ('jurors', 4), ('rightly', 1), ('men', 1), ('slates', 8), ('indignant', 1), ('spectacles', 3), ('spell', 1), ('neighbour', 1), ('muddle', 1), ('pencil', 2), ('squeaked', 1), ('quickly', 2), ('juror', 1), ('herald', 1), ('accusation', 1), ('blew', 2), ('blasts', 2), ('summer', 2), ('verdict', 4), ('witness', 10), ('teacup', 3), ('sent', 2), ('fourteenth', 1), ('fifteenth', 1), ('sixteenth', 1), ('wrote', 3), ('dates', 1), ('reduced', 1), ('shillings', 1), ('pence', 1), ('hat', 1), ('stolen', 1), ('memorandum', 1), ('fidgeted', 1), ('evidence', 7), ('spot', 1), ('encourage', 1), ('shifting', 1), ('remain', 1), ('squeeze', 1), ('boldly', 1), ('reasonable', 1), ('pace', 1), ('ridiculous', 1), ('fashion', 2), ('officers', 4), ('list', 3), ('singers', 2), ('thin', 1), ('dunce', 1), ('twinkled', 1), ('deny', 2), ('denies', 1), ('miserable', 2), ('speaker', 1), ('suppressed', 4), ('canvas', 1), ('bag', 1), ('tied', 1), ('strings', 1), ('newspapers', 1), ('trials', 1), ('attempts', 1), ('applause', 1), ('understood', 1), ('lower', 1), ('officer', 1), ('examine', 2), ('folding', 1), ('collar', 1), ('shrieked', 1), ('behead', 1), ('suppress', 1), ('undertone', 2), ('fumbled', 1), ('xii', 1), ('flurry', 1), ('tipped', 1), ('skirt', 1), ('upsetting', 1), ('jurymen', 4), ('sprawling', 1), ('reminding', 1), ('globe', 1), ('goldfish', 2), ('accidentally', 1), ('dismay', 1), ('accident', 2), ('vague', 1), ('die', 1), ('cannot', 1), ('emphasis', 1), ('haste', 1), ('downwards', 1), ('unable', 1), ('signifies', 1), ('shock', 1), ('pencils', 1), ('diligently', 1), ('overcome', 1), ('gazing', 1), ('unimportant', 5), ('respectful', 1), ('note', 2), ('cackled', 1), ('forty', 1), ('persons', 1), ('mile', 2), ('invented', 1), ('oldest', 1), ('prisoner', 2), ('directed', 2), ('verses', 4), ('handwriting', 1), ('queerest', 1), ('imitated', 1), ('prove', 1), ('signed', 2), ('sign', 1), ('mischief', 1), ('honest', 1), ('clapping', 1), ('proves', 2), ('guilt', 1), ('character', 1), ('push', 1), ('returned', 2), ('involved', 1), ('affair', 1), ('trusts', 1), ('fit', 3), ('obstacle', 1), ('ourselves', 1), ('secret', 1), ('sixpence', 1), ('atom', 2), ('attempted', 1), ('saves', 1), ('spreading', 1), ('cardboard', 1), ('clearer', 1), ('fits', 1), ('furiously', 1), ('inkstand', 1), ('ink', 1), ('trickling', 1), ('pun', 1), ('twentieth', 1), ('purple', 1), ('flying', 1), ('brushing', 1), ('fluttered', 1), ('kissed', 1), ('wonderful', 2), ('setting', 1), ('dreaming', 1), ('dreamed', 1), ('clasped', 1), ('toss', 1), ('listened', 1), ('rustled', 1), ('splashed', 1), ('neighbouring', 1), ('rattle', 1), ('teacups', 2), ('shared', 1), ('meal', 1), ('crashed', 1), ('choking', 1), ('believed', 1), ('wonderland', 2), ('reality', 1), ('rustling', 1), ('rippling', 1), ('reeds', 1), ('tinkling', 1), ('sheep', 1), ('bells', 1), ('cries', 1), ('shepherd', 1), ('noises', 1), ('clamour', 1), ('farm', 1), ('yard', 1), ('lowing', 1), ('cattle', 1), ('lastly', 1), ('pictured', 1), ('riper', 1), ('years', 1), ('loving', 1), ('childhood', 1), ('gather', 1), ('sorrows', 1), ('joys', 1), ('remembering', 1), ('happy', 1)])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6245,
     "status": "ok",
     "timestamp": 1589281913535,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "iloUdkWPkhks",
    "outputId": "3eeb2dee-6cca-485b-a8f5-4b692b4c16be"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 50)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-608bd9c79ae0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"brother\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sister\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"father\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mother\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfindAnalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1167\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    142\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m    143\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                         estimator=estimator)\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    584\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                              % (n_samples, array.shape, ensure_min_samples,\n\u001b[0;32m--> 586\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 50)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "emb = skipgram_embedding_50\n",
    "\n",
    "prediction = embed(\"brother\", emb) - embed(\"sister\", emb) + embed(\"father\", emb)\n",
    "\n",
    "print(cosine_similarity(prediction, embed(\"mother\", emb)))\n",
    "\n",
    "findAnalogy(prediction, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h-OWIF9AsKH"
   },
   "source": [
    "## Task 1.4 - Discussion\n",
    "Answer the following question:\n",
    "* Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOa7EXhD-saI"
   },
   "source": [
    "# Question 2 - Peer review (0 pt):\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member\n",
    "contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others? You can use the table below to make an overview of how the tasks were divided:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWdcoJH__MqP"
   },
   "source": [
    "| Student name | Task  |\n",
    "|------|------|\n",
    "|  student name 1  | task x |\n",
    "| student name 2  | task x|\n",
    "| everyone | task x|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ie_FQhMx6Jb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_(TF_2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
