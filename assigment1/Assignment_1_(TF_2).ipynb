{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ORVPadj7rKq"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "<b>Group 82</b>\n",
    "* <b> Student 1 </b> : Joris Willems, 0908753\n",
    "* <b> Student 2 </b> : Lars Schilders, 0908729\n",
    "\n",
    "**Reading material**\n",
    "* [1] Mikolov, Tomas, et al. \"[Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\" Advances in neural information processing systems. 2013. \n",
    "\n",
    "<b><font color='red'>NOTE</font></b> When submitting your notebook, please make sure that the training history of your model is visible in the output. This means that you should **NOT** clean your output cells of the notebook. Make sure that your notebook runs without errors in linear order.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rQwLyiMFu_1"
   },
   "source": [
    "# Question 1 - Keras implementation (10 pt)\n",
    "\n",
    "### Word embeddings\n",
    "Build word embeddings with a Keras implementation where the embedding vector is of length 50, 150 and 300. Use the Alice in Wonderland text book for training. Use a window size of 2 to train the embeddings (`window_size` in the jupyter notebook). \n",
    "\n",
    "1. Build word embeddings of length 50, 150 and 300 using the Skipgram model\n",
    "2. Build word embeddings of length 50, 150 and 300 using CBOW model\n",
    "3. Analyze the different word embeddings:\n",
    "    - Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in the paper. Do not use existing libraries for this task such as Gensim. \n",
    "Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. $e_{x}$ denotes the embedding of word $x$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.\n",
    "    - Give at least 5 different  examples of analogies.\n",
    "    - Compare the performance on the analogy tasks between the word embeddings and briefly discuss your results.\n",
    "\n",
    "4. Discuss:\n",
    "  - Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n",
    "\n",
    "\n",
    "<b>HINT</b> See practical 3.1 for some helpful code to start this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctoyAoX1AI6T"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5VOelR7BYQ1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6983,
     "status": "ok",
     "timestamp": 1589279725100,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "vCnATRPgBZEd",
    "outputId": "66766056-0360-4bcb-8240-27b98039d1a1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Reshape, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# other helpful libraries\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history(path, history):\n",
    "    import json, os\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "        \n",
    "def save_embedding(path, model):\n",
    "    np.save(path, model.get_weights()[0])\n",
    "    \n",
    "def plot_history(history, title):\n",
    "    pd.DataFrame(history.history).plot()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Cross-Entropy\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6981,
     "status": "ok",
     "timestamp": 1589279725101,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "qBNCPtOoBbB1",
    "outputId": "8cf8d48f-9b08-478c-e7a3-288bb932eaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__) #  check what version of TF is imported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WCd0zUO1AKjY"
   },
   "source": [
    "### Import file\n",
    "\n",
    "If you use Google Colab, you need to mount your Google Drive to the notebook when you want to use files that are located in your Google Drive. Paste the authorization code, from the new tab page that opens automatically when running the cell, in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24562,
     "status": "ok",
     "timestamp": 1589279742721,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "DdjNeehKBd-a",
    "outputId": "bccb459c-8be3-467d-9598-760ae04e7084"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjIVBt8YGUaO"
   },
   "source": [
    "Navigate to the folder in which `alice.txt` is located. Make sure to start path with '/content/drive/My Drive/' if you want to load the file from your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1589279763064,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "iS0-uINUBfic",
    "outputId": "a7068012-ab31-4d87-a9b8-2ac9c37fb194"
   },
   "outputs": [],
   "source": [
    "cd '/content/drive/My Drive/Colab Notebooks/2IMM10 - Deep Learning/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gz8Z5gCDBhSl"
   },
   "outputs": [],
   "source": [
    "file_name = 'alice.txt'\n",
    "corpus = open(file_name).readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zkbk32wHANnD"
   },
   "source": [
    "### Data preprocessing\n",
    "\n",
    "See Practical 3.1 for an explanation of the preprocessing steps done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37PyOHq2BkY4"
   },
   "outputs": [],
   "source": [
    "# Removes sentences with fewer than 3 words\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# remove punctuation in text and fit tokenizer on entire corpus\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# convert text to sequence of integer values\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "n_samples = sum(len(s) for s in corpus) # total number of words in the corpus\n",
    "V = len(tokenizer.word_index) + 1 # total number of unique words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1589279777863,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "ILdA_IimBlte",
    "outputId": "30756520-d777-4968-ff46-d5364fc5bf4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus: 27165 \n",
      "Number of words in vocabulary: 2557\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in corpus: {} \\nNumber of words in vocabulary: {}\".format(n_samples, V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 594,
     "status": "ok",
     "timestamp": 1589279780456,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "fRbpue0WBms6",
    "outputId": "ebbf9719-6d1f-4a02-ec75-cdbe1ecc9589"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1), ('and', 2), ('to', 3), ('a', 4), ('it', 5)]\n"
     ]
    }
   ],
   "source": [
    "# example of how word to integer mapping looks like in the tokenizer\n",
    "print(list((tokenizer.word_index.items()))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Er86VxH9BqI9"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "window_size = 2\n",
    "window_size_corpus = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0sU1JORATvX"
   },
   "source": [
    "## Task 1.1 - Skipgram\n",
    "Build word embeddings of length 50, 150 and 300 using the Skipgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Udp1xKcDBu0v"
   },
   "outputs": [],
   "source": [
    "#prepare data for skipgram\n",
    "def generate_data_skipgram(corpus, window_size, V):\n",
    "\n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            for i in range(p, n):\n",
    "                if i != index and 0 <= i < L:\n",
    "                    # Add the input word\n",
    "                    #in_words.append(word)\n",
    "                    all_in.append(word)\n",
    "                    # Add one-hot of the context words\n",
    "                    all_out.append(to_categorical(words[i], V))\n",
    "                                      \n",
    "    return (np.array(all_in),np.array(all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXBGNrKTB0tO"
   },
   "outputs": [],
   "source": [
    "# create training data\n",
    "x , y = generate_data_skipgram(corpus,window_size,V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1FTkStoDLFQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94556 samples\n",
      "Epoch 1/2\n",
      "94556/94556 [==============================] - 3s 35us/sample - loss: 5.0282\n",
      "Epoch 2/2\n",
      "94556/94556 [==============================] - 3s 35us/sample - loss: 4.9917\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create skipgram architecture\n",
    "def build_skipgram(dim):\n",
    "    skipgram = Sequential()\n",
    "    skipgram.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=1))\n",
    "    skipgram.add(Reshape((dim, )))\n",
    "    skipgram.add(Dense(V, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "    skipgram.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1))\n",
    "    return skipgram\n",
    "\n",
    "def fit_skipgram(dim, batch_size=64, epochs=20):\n",
    "    skipgram = build_skipgram(dim)\n",
    "    history = skipgram_50.fit(x, y, batch_size=batch_size, epochs=epochs)\n",
    "    save_embedding(\"embeddingSkipgram/{}.npy\".format(dim), skipgram)\n",
    "    save_history(\"embeddingSkipgram/{}.json\".format(dim), history)\n",
    "    plot_history(history, title=\"Skipgram with {}-dim embedding\".format(dim))\n",
    "  \n",
    "fit_skipgram(50, batch_size=64, epochs=15)\n",
    "fit_skipgram(100, batch_size=64, epochs=15)\n",
    "fit_skipgram(150, batch_size=64, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history, title='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94556 samples\n",
      "Epoch 1/2\n",
      "94556/94556 [==============================] - 4s 38us/sample - loss: 6.1116\n",
      "Epoch 2/2\n",
      "94556/94556 [==============================] - 3s 36us/sample - loss: 5.7631\n"
     ]
    }
   ],
   "source": [
    "skipgram_50 = build_skipgram(50)\n",
    "hist_SG_50 = skipgram_50.fit(x, y, batch_size=bs, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test.json\", 'w') as f:\n",
    "    json.dump(hist_SG_50.history, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [6.111622221049603, 5.7630591651520815]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yxb3ulOlD1Z2"
   },
   "source": [
    "<b>HINT</b>: To increase training speed of your model, you can use the free available GPU power in Google Colab. Go to `Edit` --> `Notebook Settings` --> select `GPU` under `hardware accelerator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 144525,
     "status": "ok",
     "timestamp": 1589281183271,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "a9eg0xPoDP9B",
    "outputId": "83972df0-8bf3-459c-c7f5-33bc440368bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 94556 samples\n",
      "Epoch 1/30\n",
      "44224/94556 [=============>................] - ETA: 2s - loss: 6.3257"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9efd67c0297d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mskipgram_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_skipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mhist_SG_50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskipgram_50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mskipgram_150\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_skipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mhist_SG_150\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskipgram_150\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/webret/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    590\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0;34m\"\"\"Calls the graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train skipgram model\n",
    "epochs = 30\n",
    "bs = 64\n",
    "\n",
    "skipgram_50 = build_skipgram(50)\n",
    "hist_SG_50 = skipgram_50.fit(x, y, batch_size=bs, epochs=epochs)\n",
    "skipgram_150 = build_skipgram(150)\n",
    "hist_SG_150 = skipgram_150.fit(x, y, batch_size=bs, epochs=epochs)\n",
    "skipgram_300 = build_skipgram(300)\n",
    "hist_SGF_300 = skipgram_300.fit(x, y, batch_size=bs, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGEHlkQ4DShO"
   },
   "outputs": [],
   "source": [
    "# save embeddings for vectors of length 50, 150 and 300 using skipgram model\n",
    "\n",
    "np.save(\"embeddingSkipgram/embedding50.npy\", skipgram_50.get_weights()[0])\n",
    "np.save(\"embeddingSkipgram/embedding150.npy\", skipgram_150.get_weights()[0])\n",
    "np.save(\"embeddingSkipgram/embedding300.npy\", skipgram_300.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b4z9Lt6pAZEw"
   },
   "source": [
    "## Task 1.2 - CBOW\n",
    "\n",
    "Build word embeddings of length 50, 150 and 300 using CBOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# prepare data for CBOW\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    \n",
    "    maxlen = window_size*2\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "    for words in corpus:\n",
    "        L = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            p = index - window_size\n",
    "            n = index + window_size + 1\n",
    "                    \n",
    "            in_words = []\n",
    "            labels = []\n",
    "            \n",
    "            context = [words[i] for i in range(p, n) if ((i != index) and (0 <= i < L))]\n",
    "            context = (maxlen - len(context)) * [0] + context\n",
    "            \n",
    "            all_in.append(context)\n",
    "\n",
    "            all_out.append(to_categorical(word, V))\n",
    "                                      \n",
    "    return (np.array(all_in),np.array(all_out))\n",
    "\n",
    "# create training data\n",
    "x_cbow, y_cbow = generate_data_cbow(corpus,window_size, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# create CBOW architecture\n",
    "def build_cbow(dim):\n",
    "    cbow = Sequential()\n",
    "    cbow.add(Embedding(input_dim=V, output_dim=dim, embeddings_initializer='glorot_uniform', input_length=window_size*2))\n",
    "    cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(dim,)))\n",
    "    cbow.add(Dense(V, kernel_initializer='glorot_uniform', activation='softmax'))\n",
    "    cbow.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=2))\n",
    "    return cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KMhI_sWFTXDW",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27165 samples\n",
      "Epoch 1/50\n",
      "27165/27165 [==============================] - 1s 46us/sample - loss: 7.64060s - loss: 7.\n",
      "Epoch 2/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 7.2614\n",
      "Epoch 3/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 6.98900s - loss: \n",
      "Epoch 4/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 6.8040\n",
      "Epoch 5/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.6721\n",
      "Epoch 6/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.5789\n",
      "Epoch 7/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.5095\n",
      "Epoch 8/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.4545\n",
      "Epoch 9/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.40810s - lo\n",
      "Epoch 10/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.36700\n",
      "Epoch 11/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 6.3294\n",
      "Epoch 12/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 6.2942\n",
      "Epoch 13/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.2606\n",
      "Epoch 14/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.2282\n",
      "Epoch 15/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.1971\n",
      "Epoch 16/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 6.1670\n",
      "Epoch 17/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.1384\n",
      "Epoch 18/50\n",
      "27165/27165 [==============================] - 1s 43us/sample - loss: 6.1114\n",
      "Epoch 19/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.08620s - loss: \n",
      "Epoch 20/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 6.0632\n",
      "Epoch 21/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.0420\n",
      "Epoch 22/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 6.0225\n",
      "Epoch 23/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 6.0046\n",
      "Epoch 24/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.9880\n",
      "Epoch 25/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.9725\n",
      "Epoch 26/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.9578\n",
      "Epoch 27/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.9439\n",
      "Epoch 28/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.9306\n",
      "Epoch 29/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.91760s \n",
      "Epoch 30/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.90520s - loss: 5.906\n",
      "Epoch 31/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.8930\n",
      "Epoch 32/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.8809\n",
      "Epoch 33/50\n",
      "27165/27165 [==============================] - 1s 41us/sample - loss: 5.8690\n",
      "Epoch 34/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.8572\n",
      "Epoch 35/50\n",
      "27165/27165 [==============================] - 1s 42us/sample - loss: 5.84540s - loss\n",
      "Epoch 36/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.8335\n",
      "Epoch 37/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.8217\n",
      "Epoch 38/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.8098\n",
      "Epoch 39/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.7979\n",
      "Epoch 40/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.7860\n",
      "Epoch 41/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.7740\n",
      "Epoch 42/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.7622\n",
      "Epoch 43/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.75020s - loss: 5\n",
      "Epoch 44/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.7382\n",
      "Epoch 45/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.7266\n",
      "Epoch 46/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.7148\n",
      "Epoch 47/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.7031\n",
      "Epoch 48/50\n",
      "27165/27165 [==============================] - 1s 38us/sample - loss: 5.6913\n",
      "Epoch 49/50\n",
      "27165/27165 [==============================] - 1s 40us/sample - loss: 5.6798\n",
      "Epoch 50/50\n",
      "27165/27165 [==============================] - 1s 39us/sample - loss: 5.6683\n",
      "Train on 27165 samples\n",
      "Epoch 1/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 7.6397\n",
      "Epoch 2/50\n",
      "27165/27165 [==============================] - 1s 54us/sample - loss: 7.2582\n",
      "Epoch 3/50\n",
      "27165/27165 [==============================] - 1s 55us/sample - loss: 6.9842\n",
      "Epoch 4/50\n",
      "27165/27165 [==============================] - 1s 54us/sample - loss: 6.79590s\n",
      "Epoch 5/50\n",
      "27165/27165 [==============================] - 1s 53us/sample - loss: 6.6612\n",
      "Epoch 6/50\n",
      "27165/27165 [==============================] - 1s 54us/sample - loss: 6.5651\n",
      "Epoch 7/50\n",
      "27165/27165 [==============================] - 2s 55us/sample - loss: 6.4930\n",
      "Epoch 8/50\n",
      "27165/27165 [==============================] - 2s 57us/sample - loss: 6.4346\n",
      "Epoch 9/50\n",
      "27165/27165 [==============================] - 2s 55us/sample - loss: 6.3840\n",
      "Epoch 10/50\n",
      "27165/27165 [==============================] - 2s 56us/sample - loss: 6.3379\n",
      "Epoch 11/50\n",
      "27165/27165 [==============================] - 2s 56us/sample - loss: 6.2950\n",
      "Epoch 12/50\n",
      "27165/27165 [==============================] - 2s 56us/sample - loss: 6.2548\n",
      "Epoch 13/50\n",
      "27165/27165 [==============================] - 2s 55us/sample - loss: 6.2171\n",
      "Epoch 14/50\n",
      "27165/27165 [==============================] - 2s 57us/sample - loss: 6.1816\n",
      "Epoch 15/50\n",
      "27165/27165 [==============================] - ETA: 0s - loss: 6.146 - 2s 57us/sample - loss: 6.1487\n",
      "Epoch 16/50\n",
      "27165/27165 [==============================] - 2s 58us/sample - loss: 6.1182\n",
      "Epoch 17/50\n",
      "27165/27165 [==============================] - 2s 57us/sample - loss: 6.0903\n",
      "Epoch 18/50\n",
      "27165/27165 [==============================] - 2s 59us/sample - loss: 6.0647\n",
      "Epoch 19/50\n",
      "27165/27165 [==============================] - 2s 58us/sample - loss: 6.0414\n",
      "Epoch 20/50\n",
      "27165/27165 [==============================] - 2s 58us/sample - loss: 6.0200\n",
      "Epoch 21/50\n",
      "27165/27165 [==============================] - 2s 59us/sample - loss: 6.0002\n",
      "Epoch 22/50\n",
      "27165/27165 [==============================] - 2s 62us/sample - loss: 5.9817\n",
      "Epoch 23/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.9641\n",
      "Epoch 24/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.9475\n",
      "Epoch 25/50\n",
      "27165/27165 [==============================] - 2s 64us/sample - loss: 5.9314\n",
      "Epoch 26/50\n",
      "27165/27165 [==============================] - 2s 61us/sample - loss: 5.9159\n",
      "Epoch 27/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.9007\n",
      "Epoch 28/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.8858\n",
      "Epoch 29/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.8711\n",
      "Epoch 30/50\n",
      "27165/27165 [==============================] - 2s 61us/sample - loss: 5.8567\n",
      "Epoch 31/50\n",
      "27165/27165 [==============================] - 2s 60us/sample - loss: 5.8420\n",
      "Epoch 32/50\n",
      "27165/27165 [==============================] - 2s 58us/sample - loss: 5.8277\n",
      "Epoch 33/50\n",
      "27165/27165 [==============================] - 2s 61us/sample - loss: 5.8133\n",
      "Epoch 34/50\n",
      "27165/27165 [==============================] - 2s 64us/sample - loss: 5.7990\n",
      "Epoch 35/50\n",
      "27165/27165 [==============================] - 2s 65us/sample - loss: 5.7847\n",
      "Epoch 36/50\n",
      "27165/27165 [==============================] - 2s 62us/sample - loss: 5.7704\n",
      "Epoch 37/50\n",
      "27165/27165 [==============================] - 2s 63us/sample - loss: 5.7564\n",
      "Epoch 38/50\n",
      "27165/27165 [==============================] - 2s 63us/sample - loss: 5.7424\n",
      "Epoch 39/50\n",
      "27165/27165 [==============================] - 2s 62us/sample - loss: 5.7284\n",
      "Epoch 40/50\n",
      "27165/27165 [==============================] - 2s 63us/sample - loss: 5.7148\n",
      "Epoch 41/50\n",
      "27165/27165 [==============================] - 2s 63us/sample - loss: 5.7009\n",
      "Epoch 42/50\n",
      "27165/27165 [==============================] - 2s 64us/sample - loss: 5.6875\n",
      "Epoch 43/50\n",
      "27165/27165 [==============================] - 2s 62us/sample - loss: 5.6743\n",
      "Epoch 44/50\n",
      "27165/27165 [==============================] - 2s 65us/sample - loss: 5.6612\n",
      "Epoch 45/50\n",
      "27165/27165 [==============================] - 2s 66us/sample - loss: 5.6480\n",
      "Epoch 46/50\n",
      "27165/27165 [==============================] - 2s 62us/sample - loss: 5.6354\n",
      "Epoch 47/50\n",
      "27165/27165 [==============================] - 2s 65us/sample - loss: 5.6226\n",
      "Epoch 48/50\n",
      "27165/27165 [==============================] - 2s 66us/sample - loss: 5.6102\n",
      "Epoch 49/50\n",
      "27165/27165 [==============================] - 2s 63us/sample - loss: 5.5978\n",
      "Epoch 50/50\n",
      "27165/27165 [==============================] - 2s 64us/sample - loss: 5.5856\n",
      "Train on 27165 samples\n",
      "Epoch 1/50\n",
      "27165/27165 [==============================] - 4s 135us/sample - loss: 7.6385\n",
      "Epoch 2/50\n",
      "27165/27165 [==============================] - 4s 137us/sample - loss: 7.2546\n",
      "Epoch 3/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 6.9778\n",
      "Epoch 4/50\n",
      "27165/27165 [==============================] - 3s 128us/sample - loss: 6.7864\n",
      "Epoch 5/50\n",
      "27165/27165 [==============================] - 4s 130us/sample - loss: 6.6489\n",
      "Epoch 6/50\n",
      "27165/27165 [==============================] - 4s 140us/sample - loss: 6.5508\n",
      "Epoch 7/50\n",
      "27165/27165 [==============================] - 4s 143us/sample - loss: 6.4764\n",
      "Epoch 8/50\n",
      "27165/27165 [==============================] - 4s 131us/sample - loss: 6.4149\n",
      "Epoch 9/50\n",
      "27165/27165 [==============================] - 4s 135us/sample - loss: 6.3601\n",
      "Epoch 10/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 6.3098\n",
      "Epoch 11/50\n",
      "27165/27165 [==============================] - 4s 131us/sample - loss: 6.2633\n",
      "Epoch 12/50\n",
      "27165/27165 [==============================] - 3s 128us/sample - loss: 6.2205\n",
      "Epoch 13/50\n",
      "27165/27165 [==============================] - 4s 130us/sample - loss: 6.1812\n",
      "Epoch 14/50\n",
      "27165/27165 [==============================] - 3s 127us/sample - loss: 6.1452\n",
      "Epoch 15/50\n",
      "27165/27165 [==============================] - 4s 132us/sample - loss: 6.1123\n",
      "Epoch 16/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 6.0825\n",
      "Epoch 17/50\n",
      "27165/27165 [==============================] - 4s 130us/sample - loss: 6.0554\n",
      "Epoch 18/50\n",
      "27165/27165 [==============================] - 4s 131us/sample - loss: 6.0305\n",
      "Epoch 19/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 6.0077\n",
      "Epoch 20/50\n",
      "27165/27165 [==============================] - 4s 135us/sample - loss: 5.9862\n",
      "Epoch 21/50\n",
      "27165/27165 [==============================] - 4s 130us/sample - loss: 5.9660\n",
      "Epoch 22/50\n",
      "27165/27165 [==============================] - 4s 131us/sample - loss: 5.9467\n",
      "Epoch 23/50\n",
      "27165/27165 [==============================] - 4s 134us/sample - loss: 5.9282\n",
      "Epoch 24/50\n",
      "27165/27165 [==============================] - 4s 135us/sample - loss: 5.9103\n",
      "Epoch 25/50\n",
      "27165/27165 [==============================] - 4s 134us/sample - loss: 5.8929\n",
      "Epoch 26/50\n",
      "27165/27165 [==============================] - 4s 134us/sample - loss: 5.8758\n",
      "Epoch 27/50\n",
      "27165/27165 [==============================] - 4s 134us/sample - loss: 5.8589\n",
      "Epoch 28/50\n",
      "27165/27165 [==============================] - 4s 132us/sample - loss: 5.8424\n",
      "Epoch 29/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 5.8259\n",
      "Epoch 30/50\n",
      "27165/27165 [==============================] - 4s 144us/sample - loss: 5.8098\n",
      "Epoch 31/50\n",
      "27165/27165 [==============================] - 4s 145us/sample - loss: 5.7936\n",
      "Epoch 32/50\n",
      "27165/27165 [==============================] - 4s 142us/sample - loss: 5.7778\n",
      "Epoch 33/50\n",
      "27165/27165 [==============================] - 4s 147us/sample - loss: 5.7622\n",
      "Epoch 34/50\n",
      "27165/27165 [==============================] - 4s 139us/sample - loss: 5.7467\n",
      "Epoch 35/50\n",
      "27165/27165 [==============================] - 4s 136us/sample - loss: 5.7313\n",
      "Epoch 36/50\n",
      "27165/27165 [==============================] - 4s 134us/sample - loss: 5.7162\n",
      "Epoch 37/50\n",
      "27165/27165 [==============================] - 4s 138us/sample - loss: 5.7015\n",
      "Epoch 38/50\n",
      "27165/27165 [==============================] - 4s 140us/sample - loss: 5.6868\n",
      "Epoch 39/50\n",
      "27165/27165 [==============================] - 4s 139us/sample - loss: 5.6724\n",
      "Epoch 40/50\n",
      "27165/27165 [==============================] - 4s 145us/sample - loss: 5.6581\n",
      "Epoch 41/50\n",
      "27165/27165 [==============================] - 4s 145us/sample - loss: 5.6442\n",
      "Epoch 42/50\n",
      "27165/27165 [==============================] - 4s 147us/sample - loss: 5.6304\n",
      "Epoch 43/50\n",
      "27165/27165 [==============================] - 4s 138us/sample - loss: 5.6168\n",
      "Epoch 44/50\n",
      "27165/27165 [==============================] - 4s 139us/sample - loss: 5.6035\n",
      "Epoch 45/50\n",
      "27165/27165 [==============================] - 4s 133us/sample - loss: 5.5902\n",
      "Epoch 46/50\n",
      "27165/27165 [==============================] - 4s 136us/sample - loss: 5.5773\n",
      "Epoch 47/50\n",
      "27165/27165 [==============================] - 4s 140us/sample - loss: 5.5647\n",
      "Epoch 48/50\n",
      "27165/27165 [==============================] - 4s 141us/sample - loss: 5.5520\n",
      "Epoch 49/50\n",
      "27165/27165 [==============================] - 4s 138us/sample - loss: 5.5396\n",
      "Epoch 50/50\n",
      "27165/27165 [==============================] - 4s 137us/sample - loss: 5.5276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe1199d6e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train CBOW model\n",
    "epochs = 50\n",
    "bs = 64\n",
    "\n",
    "cbow_50 = build_cbow(50)\n",
    "hist_cbow_50 = cbow_50.fit(x_cbow, y_cbow, batch_size=bs, epochs=epochs)\n",
    "cbow_150 = build_cbow(150)\n",
    "hist_cbow_150 = cbow_150.fit(x_cbow, y_cbow, batch_size=bs, epochs=epochs)\n",
    "cbow_300 = build_cbow(300)\n",
    "hist_cbow_300 = cbow_300.fit(x_cbow, y_cbow, batch_size=bs, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KMhI_sWFTXDW"
   },
   "outputs": [],
   "source": [
    "# save embeddings for vectors of length 50, 150 and 300 using CBOW model\n",
    "np.save(\"embeddingCBOW/embedding50.npy\", cbow_50.get_weights()[0])\n",
    "np.save(\"embeddingCBOW/embedding150.npy\", cbow_150.get_weights()[0])\n",
    "np.save(\"embeddingCBOW/embedding300.npy\", cbow_300.get_weights()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rr2BAI4nAfnG"
   },
   "source": [
    "## Task 1.3 - Analogy function\n",
    "\n",
    "Implement your own function to perform the analogy task (see [1] for concrete examples). Use the same distance metric as in [1]. Do not use existing libraries for this task such as Gensim. Your function should be able to answer whether an analogy like: \"a king is to a queen as a man is to a woman\" ($e_{king} - e_{queen} + e_{woman} \\approx e_{man}$) is true. \n",
    "\n",
    "In a perfect scenario, we would like that this analogy ( $e_{king} - e_{queen} + e_{woman}$) results in the embedding of the word \"man\". However, it does not always result in exactly the same word embedding. The result of the formula is called the expected or the predicted word embedding. In this context, \"man\" is called the true or the actual word $t$. We want to find the word $p$ in the vocabulary, where the embedding of $p$ ($e_p$) is the closest to the predicted embedding (i.e. result of the formula). Then, we can check if $p$ is the same word as the true word $t$.  \n",
    "\n",
    "You have to answer an analogy function using each embedding for both CBOW and Skipgram model. This means that for each analogy we have 6 outputs. Show the true word (with distance similarity value between predicted embedding and true word embedding, i.e. `sim1`) , the predicted word (with distance similarity value between predicted embedding and the embedding of the word in the vocabulary that is closest to this predicted embedding, i.e. `sim2`) and a boolean answer whether the predicted word **exactly** equals the true word. \n",
    "\n",
    "<b>HINT</b>: to visualize the results of the analogy tasks , you can print them in a table. An example is given below.\n",
    "\n",
    "\n",
    "| Analogy task | True word (sim1)  | Predicted word (sim2) | Embedding | Correct?|\n",
    "|------|------|------|------|------|\n",
    "|  queen is to king as woman is to ?\t | man (sim1) | predictd_word(sim2) | SG_50 | True / False|\n",
    "\n",
    "* Give at least 5 different  examples of analogies.\n",
    "* Compare the performance on the analogy s between the word embeddings and briefly discuss your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_embedding_50 = np.load('embeddingSkipgram/embedding50.npy')\n",
    "skipgram_embedding_150 = np.load('embeddingSkipgram/embedding150.npy')\n",
    "skipgram_embedding_300 = np.load('embeddingSkipgram/embedding300.npy')\n",
    "\n",
    "cbow_embedding_50 = np.load('embeddingCBOW/embedding50.npy')\n",
    "cbow_embedding_150 = np.load('embeddingCBOW/embedding150.npy')\n",
    "cbow_embedding_300 = np.load('embeddingCBOW/embedding300.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dict = {'SG_50': skipgram_embedding_50, 'SG_150': skipgram_embedding_150, 'SG_300': skipgram_embedding_300,\n",
    "           'CBOW_50': cbow_embedding_50, 'CBOW_150': cbow_embedding_150, 'CBOW_300': cbow_embedding_300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7vMhZ0LitFC"
   },
   "outputs": [],
   "source": [
    "def embed(word, embedding, vocab_size = V, tokenizer=tokenizer):\n",
    "    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
    "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
    "    # get the one-hot encoding of the word\n",
    "    bin_word = to_categorical(int_word, V)\n",
    "    return np.dot(bin_word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def findAnalogy(prediction, embedding):\n",
    "    \n",
    "    all_word_embeddings = embed(list(tokenizer.word_index.keys()), embedding)\n",
    "    \n",
    "    similarity = cosine_similarity(all_word_embeddings, prediction)\n",
    "\n",
    "    word_idx = similarity.argsort(axis=0)[-2,0]\n",
    "\n",
    "    return list(tokenizer.word_index.keys())[word_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6245,
     "status": "ok",
     "timestamp": 1589281913535,
     "user": {
      "displayName": "Lars Schilders",
      "photoUrl": "",
      "userId": "04132714612324641452"
     },
     "user_tz": -120
    },
    "id": "iloUdkWPkhks",
    "outputId": "3eeb2dee-6cca-485b-a8f5-4b692b4c16be"
   },
   "outputs": [],
   "source": [
    "def analogyOutput(emb_name, relation_1, relation_2):\n",
    "    \n",
    "    emb = emb_dict[emb_name]\n",
    "    \n",
    "    analogy_task = \"{} is to {} as {} is to ?\".format(relation_1[0], relation_1[1], relation_2[0], relation_2[1])\n",
    "    \n",
    "    prediction = embed(relation_1[0], emb) - embed(relation_1[1], emb) + embed(relation_2[0], emb)\n",
    "    \n",
    "    predicted_word = findAnalogy(prediction, emb)\n",
    "    sim2 = cosine_similarity(prediction, embed(predicted_word, emb))[0,0]\n",
    "    \n",
    "    true_word = relation_2[1]\n",
    "    sim1 = cosine_similarity(prediction, embed(true_word, emb))[0,0]\n",
    "    \n",
    "    correct = true_word == predicted_word\n",
    "    \n",
    "    return [analogy_task, true_word, sim1, predicted_word, sim2, emb_name, correct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy task</th>\n",
       "      <th>True word</th>\n",
       "      <th>sim1</th>\n",
       "      <th>Predicted word</th>\n",
       "      <th>sim2</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Correct?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>0.142158</td>\n",
       "      <td>sleepy</td>\n",
       "      <td>0.497267</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>-0.080373</td>\n",
       "      <td>up</td>\n",
       "      <td>0.459103</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>-0.039956</td>\n",
       "      <td>up</td>\n",
       "      <td>0.480250</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>0.065749</td>\n",
       "      <td>behind</td>\n",
       "      <td>0.500375</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>0.012946</td>\n",
       "      <td>high</td>\n",
       "      <td>0.448162</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>up is to down as high is to ?</td>\n",
       "      <td>low</td>\n",
       "      <td>-0.002117</td>\n",
       "      <td>high</td>\n",
       "      <td>0.423648</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>-0.238600</td>\n",
       "      <td>from</td>\n",
       "      <td>0.533501</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>-0.015764</td>\n",
       "      <td>everything</td>\n",
       "      <td>0.544888</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>-0.040131</td>\n",
       "      <td>everything</td>\n",
       "      <td>0.564488</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>0.157352</td>\n",
       "      <td>everything</td>\n",
       "      <td>0.489072</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>0.037045</td>\n",
       "      <td>always</td>\n",
       "      <td>0.419925</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>everything is to nothing as always is to ?</td>\n",
       "      <td>never</td>\n",
       "      <td>-0.018740</td>\n",
       "      <td>always</td>\n",
       "      <td>0.467155</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.147854</td>\n",
       "      <td>king</td>\n",
       "      <td>0.901518</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.102285</td>\n",
       "      <td>king</td>\n",
       "      <td>0.612152</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.140928</td>\n",
       "      <td>king</td>\n",
       "      <td>0.576836</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.031550</td>\n",
       "      <td>king</td>\n",
       "      <td>0.738240</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.038164</td>\n",
       "      <td>king</td>\n",
       "      <td>0.711501</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>king is to man as queen is to ?</td>\n",
       "      <td>woman</td>\n",
       "      <td>-0.050481</td>\n",
       "      <td>king</td>\n",
       "      <td>0.710403</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Analogy task True word      sim1  \\\n",
       "0                up is to down as high is to ?       low  0.142158   \n",
       "1                up is to down as high is to ?       low -0.080373   \n",
       "2                up is to down as high is to ?       low -0.039956   \n",
       "3                up is to down as high is to ?       low  0.065749   \n",
       "4                up is to down as high is to ?       low  0.012946   \n",
       "5                up is to down as high is to ?       low -0.002117   \n",
       "6   everything is to nothing as always is to ?     never -0.238600   \n",
       "7   everything is to nothing as always is to ?     never -0.015764   \n",
       "8   everything is to nothing as always is to ?     never -0.040131   \n",
       "9   everything is to nothing as always is to ?     never  0.157352   \n",
       "10  everything is to nothing as always is to ?     never  0.037045   \n",
       "11  everything is to nothing as always is to ?     never -0.018740   \n",
       "12             king is to man as queen is to ?     woman -0.147854   \n",
       "13             king is to man as queen is to ?     woman -0.102285   \n",
       "14             king is to man as queen is to ?     woman -0.140928   \n",
       "15             king is to man as queen is to ?     woman -0.031550   \n",
       "16             king is to man as queen is to ?     woman -0.038164   \n",
       "17             king is to man as queen is to ?     woman -0.050481   \n",
       "\n",
       "   Predicted word      sim2 Embedding Correct?  \n",
       "0          sleepy  0.497267     SG_50    False  \n",
       "1              up  0.459103    SG_150    False  \n",
       "2              up  0.480250    SG_300    False  \n",
       "3          behind  0.500375   CBOW_50    False  \n",
       "4            high  0.448162  CBOW_150    False  \n",
       "5            high  0.423648  CBOW_300    False  \n",
       "6            from  0.533501     SG_50    False  \n",
       "7      everything  0.544888    SG_150    False  \n",
       "8      everything  0.564488    SG_300    False  \n",
       "9      everything  0.489072   CBOW_50    False  \n",
       "10         always  0.419925  CBOW_150    False  \n",
       "11         always  0.467155  CBOW_300    False  \n",
       "12           king  0.901518     SG_50    False  \n",
       "13           king  0.612152    SG_150    False  \n",
       "14           king  0.576836    SG_300    False  \n",
       "15           king  0.738240   CBOW_50    False  \n",
       "16           king  0.711501  CBOW_150    False  \n",
       "17           king  0.710403  CBOW_300    False  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['Analogy task', 'True word', 'sim1', 'Predicted word' ,'sim2', 'Embedding', 'Correct?'])\n",
    "\n",
    "firstRelations = [[\"up\", \"down\"], [\"everything\",\"nothing\"], [\"king\", \"man\"]]\n",
    "secondRelations = [[\"high\", \"low\"], [\"always\", \"never\"], [\"queen\", \"woman\"]]\n",
    "\n",
    "for rel_1, rel_2 in zip(firstRelations, secondRelations):\n",
    "    \n",
    "    for emb_name in emb_dict:\n",
    "\n",
    "        output = analogyOutput(emb_name, rel_1, rel_2)\n",
    "        df = df.append(pd.Series(output, index=df.columns), ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy task</th>\n",
       "      <th>True word</th>\n",
       "      <th>sim1</th>\n",
       "      <th>Predicted word</th>\n",
       "      <th>sim2</th>\n",
       "      <th>Embedding</th>\n",
       "      <th>Correct?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>0.260575</td>\n",
       "      <td>he</td>\n",
       "      <td>0.768761</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>-0.024845</td>\n",
       "      <td>she</td>\n",
       "      <td>0.554767</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>-0.028425</td>\n",
       "      <td>she</td>\n",
       "      <td>0.573049</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>-0.115868</td>\n",
       "      <td>he</td>\n",
       "      <td>0.662769</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>-0.081710</td>\n",
       "      <td>he</td>\n",
       "      <td>0.668650</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>he is to him as she is to ?</td>\n",
       "      <td>her</td>\n",
       "      <td>-0.081410</td>\n",
       "      <td>she</td>\n",
       "      <td>0.669001</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>-0.042818</td>\n",
       "      <td>mabel</td>\n",
       "      <td>0.729473</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>0.010355</td>\n",
       "      <td>eye</td>\n",
       "      <td>0.560499</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>0.057661</td>\n",
       "      <td>eye</td>\n",
       "      <td>0.600743</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>0.122762</td>\n",
       "      <td>word</td>\n",
       "      <td>0.561993</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>0.041161</td>\n",
       "      <td>word</td>\n",
       "      <td>0.502455</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>eye is to eyes as word is to ?</td>\n",
       "      <td>words</td>\n",
       "      <td>0.032078</td>\n",
       "      <td>word</td>\n",
       "      <td>0.483835</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>0.205078</td>\n",
       "      <td>me</td>\n",
       "      <td>0.706704</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>0.062404</td>\n",
       "      <td>do</td>\n",
       "      <td>0.603889</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>0.038352</td>\n",
       "      <td>do</td>\n",
       "      <td>0.620704</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>-0.035347</td>\n",
       "      <td>do</td>\n",
       "      <td>0.581160</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>0.029087</td>\n",
       "      <td>go</td>\n",
       "      <td>0.583120</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>go is to went as do is to ?</td>\n",
       "      <td>did</td>\n",
       "      <td>0.061798</td>\n",
       "      <td>go</td>\n",
       "      <td>0.550555</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>0.281980</td>\n",
       "      <td>is</td>\n",
       "      <td>0.759913</td>\n",
       "      <td>SG_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>know</td>\n",
       "      <td>0.506507</td>\n",
       "      <td>SG_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>-0.044304</td>\n",
       "      <td>know</td>\n",
       "      <td>0.497205</td>\n",
       "      <td>SG_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>0.187789</td>\n",
       "      <td>know</td>\n",
       "      <td>0.548451</td>\n",
       "      <td>CBOW_50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>know</td>\n",
       "      <td>0.591736</td>\n",
       "      <td>CBOW_150</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>know is to knew as is is to ?</td>\n",
       "      <td>was</td>\n",
       "      <td>0.036852</td>\n",
       "      <td>know</td>\n",
       "      <td>0.584038</td>\n",
       "      <td>CBOW_300</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Analogy task True word      sim1 Predicted word  \\\n",
       "0      he is to him as she is to ?       her  0.260575             he   \n",
       "1      he is to him as she is to ?       her -0.024845            she   \n",
       "2      he is to him as she is to ?       her -0.028425            she   \n",
       "3      he is to him as she is to ?       her -0.115868             he   \n",
       "4      he is to him as she is to ?       her -0.081710             he   \n",
       "5      he is to him as she is to ?       her -0.081410            she   \n",
       "6   eye is to eyes as word is to ?     words -0.042818          mabel   \n",
       "7   eye is to eyes as word is to ?     words  0.010355            eye   \n",
       "8   eye is to eyes as word is to ?     words  0.057661            eye   \n",
       "9   eye is to eyes as word is to ?     words  0.122762           word   \n",
       "10  eye is to eyes as word is to ?     words  0.041161           word   \n",
       "11  eye is to eyes as word is to ?     words  0.032078           word   \n",
       "12     go is to went as do is to ?       did  0.205078             me   \n",
       "13     go is to went as do is to ?       did  0.062404             do   \n",
       "14     go is to went as do is to ?       did  0.038352             do   \n",
       "15     go is to went as do is to ?       did -0.035347             do   \n",
       "16     go is to went as do is to ?       did  0.029087             go   \n",
       "17     go is to went as do is to ?       did  0.061798             go   \n",
       "18   know is to knew as is is to ?       was  0.281980             is   \n",
       "19   know is to knew as is is to ?       was  0.002911           know   \n",
       "20   know is to knew as is is to ?       was -0.044304           know   \n",
       "21   know is to knew as is is to ?       was  0.187789           know   \n",
       "22   know is to knew as is is to ?       was  0.020203           know   \n",
       "23   know is to knew as is is to ?       was  0.036852           know   \n",
       "\n",
       "        sim2 Embedding Correct?  \n",
       "0   0.768761     SG_50    False  \n",
       "1   0.554767    SG_150    False  \n",
       "2   0.573049    SG_300    False  \n",
       "3   0.662769   CBOW_50    False  \n",
       "4   0.668650  CBOW_150    False  \n",
       "5   0.669001  CBOW_300    False  \n",
       "6   0.729473     SG_50    False  \n",
       "7   0.560499    SG_150    False  \n",
       "8   0.600743    SG_300    False  \n",
       "9   0.561993   CBOW_50    False  \n",
       "10  0.502455  CBOW_150    False  \n",
       "11  0.483835  CBOW_300    False  \n",
       "12  0.706704     SG_50    False  \n",
       "13  0.603889    SG_150    False  \n",
       "14  0.620704    SG_300    False  \n",
       "15  0.581160   CBOW_50    False  \n",
       "16  0.583120  CBOW_150    False  \n",
       "17  0.550555  CBOW_300    False  \n",
       "18  0.759913     SG_50    False  \n",
       "19  0.506507    SG_150    False  \n",
       "20  0.497205    SG_300    False  \n",
       "21  0.548451   CBOW_50    False  \n",
       "22  0.591736  CBOW_150    False  \n",
       "23  0.584038  CBOW_300    False  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = ['Analogy task', 'True word', 'sim1', 'Predicted word' ,'sim2', 'Embedding', 'Correct?'])\n",
    "\n",
    "firstRelations = [[\"he\", \"him\"], [\"eye\", \"eyes\"], [\"go\", \"went\"],  [\"know\", \"knew\"]]\n",
    "secondRelations = [[\"she\", \"her\"], [\"word\", \"words\"], [\"do\", \"did\"], [\"is\", \"was\"]]\n",
    "\n",
    "for rel_1, rel_2 in zip(firstRelations, secondRelations):\n",
    "    \n",
    "    for emb_name in emb_dict:\n",
    "\n",
    "        output = analogyOutput(emb_name, rel_1, rel_2)\n",
    "        df = df.append(pd.Series(output, index=df.columns), ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5h-OWIF9AsKH"
   },
   "source": [
    "## Task 1.4 - Discussion\n",
    "Answer the following question:\n",
    "* Given the same number of sentences as input, CBOW and Skipgram arrange the data into different number of training samples. Which one has more and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "The skipgram model arranges the data in such a way that a certain word gets matched with its context words. More specifically, every word gets paired up with all the words in its defined context (*window_size* words in front of the word and *window_size* words behind the word), such that the training samples consist of word pairs, i.e. [(word, context_word1), (word, context_word2), (word, context_word3), (word, context_word4)] for a window_size of 2. \n",
    "\n",
    "On the contrary, the CBOW model matches the whole context of a word to that word itself. The embedding of all the words in the context gets averaged to obtain one embedding of the complete context. The training sample for one word takes the form ([context_word1, context_word2, context_word3, context_word4], word). \n",
    "\n",
    "Note that one single word in the corpus results in the generation of 2 * *window_size* training samples for the Skipgram model, whereas for the CBOW model one single word results in the generation of only 1 training sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mOa7EXhD-saI"
   },
   "source": [
    "# Question 2 - Peer review (0 pt):\n",
    "Finally, each group member must write a single paragraph outlining their opinion on the work distribution within the group. Did every group member\n",
    "contribute equally? Did you split up tasks in a fair manner, or jointly worked through the exercises. Do you think that some members of your group deserve a different grade from others? You can use the table below to make an overview of how the tasks were divided:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RWdcoJH__MqP"
   },
   "source": [
    "| Student name | Task  |\n",
    "|------|------|\n",
    "|  student name 1  | task x |\n",
    "| student name 2  | task x|\n",
    "| everyone | task x|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ie_FQhMx6Jb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_1_(TF_2).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
